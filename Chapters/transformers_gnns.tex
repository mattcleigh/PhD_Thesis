Machine learning (ML) has become a powerful tool in HEP in recent years.
ML has also been evolving rapidly, with new algorithms and techniques being developed at a breakneck pace.
The observed performance gain in many algorithms and operations used in particle physics that have adopted ML has been well documented and hard to ignore.
One of the most successful stories has been using deep learning for flavor tagging, identifying the flavor of quark that initiated a jet in the ATLAS experiment at the LHC.
However, the field of HEP has unique challenges for ML adoption, such as the requirement for explainability and robustness.
This chapter introduces relevant concepts required to understand the latest developments in the models used in flavor tagging at ATLAS: the description of deep learning, the motivation of graph neural networks, and the implementation of a transformer model.

\chapter{Deep Learning Basics}

Deep learning is the subset of machine learning that focuses on the development and training of artificial neural networks.
These neural networks take the form of a series of composable, parameterized, and differentiable transformations that map input data to output data.
Training or optimization of the network is done by modifying the parameters of each transformation to minimize some error metric over a dataset.
This is typically performed using gradient-based algorithms.
The exact form of how the loss is calculated differentiates the various types of learning, such as supervised, unsupervised, self-supervised, reinforcement, etc\footnote{There is often overlap between these categories}.

Initially, artificial neural networks were inspired by the structure of the brain, with many layers of interconnected artificial neurons processing information and sending signals to each other.
While individual neurons are simple, the emergent behaviour of the network can be complex.
As the field has evolved, the initial biological inspiration of neural networks has given way to more abstract, mathematical descriptions.
Today, network design is driven by inductive biases, computational efficiency, observations in training dynamics, and (above all) empirical results.
From this more practical perspective, an artificial neural network is long computational graph composed of differentiable operations arranged in layers that transform between collections of real-valued tensors.
The ``depth'' of a deep learning model refers to the number of layers used in the network.

The primary distinction between deep learning and other parameterized curve fitting methods is a matter of scale.
Modern networks now contain billions of parameters, requiring large datasets, specialized hardware, and sophisticated optimization algorithms to train effectively.
As the networks have grown in size and complexity, model interpretability has become a significant challenge and an active area of research.

\section{Supervised Learning}

Supervised Learning is often introduced first in a pedagogical setting, as it is the simplest and most intuitive form of machine learning. While it is not limited to deep learning, it is introduced here within its context as it is arguably the most common form of training for deep models. Indeed, many other training frameworks, such as reinforcement learning, are reframed as supervised problems to facilitate training.

In supervised learning, a single data sample is a coupling of two variables, $(x, y) \sim p(X, Y)$ is drawn from unknown joint distribution.
Here $x$ is a given observable and $y$ is the target variable.
The goal of supervised learning is to produce a discriminative model which approximates the conditional probability distribution of the target given an observation $f(x) \approx p(Y|X=x)$.
Discriminative models which permit sampling from this distribution can also be considered generative models, which are covered in \Cref{sec:generative}.
Alternatively, discriminative models can be non-probabilistic, where they simply attempt to create a mapping from inputs to outputs, $f: X \mapsto Y$.

Supervised learning typically requires a training set $\mathcal{D}$ containing of $N$ observations and their paired targets $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$.
These samples are usually assumed to be independent and identically distributed (i.i.d.) from the joint distribution $p(X, Y)$\footnote{Non i.i.d. data can sometimes be used by incorporating the appropriate weights during training}.
A supervised learning algorithm $f$, probabilistic or not, is selected out of some hypothesis space $\mathcal{H}$ to minimize some cost function $\mathcal{R}(Y \times Y) \mapsto \mathbb{R}$, which is typically a measure of empirical risk over the training set.
\begin{equation}
    f = \argmin_{f \in \mathcal{H}} \frac{1}{N} \sum_{i=1}^N \mathcal{R}(\hat{y}_i = f(x_i), y_i).
\end{equation}

For now, we make no assumptions about the structure of the input or output samples except that they can be represented as tensors, or collections of tensors, of real numbers.
Common types of supervised algorithms $f$ including support vector machines, decision trees, and of course neural networks.
For a neural network $f_{\theta}$, the hypothesis space $\mathcal{H}$ is the set of all possible network architectures and network parameter values $\theta$.

\section{Network Design}

A single network layer is any function with parameters $\theta$ that can takes in a set of input tensors and returns a set of output tensors (it could be a set of size one).
There are very few limits on what exactly this function can be, but there are some core design principles ML engineers have to rely on when designing a network to solve a specific task.
These principles are called inductive biases~\cite{InductiveBiases1, InductiveBiases2}, and they are the assumptions made which allow a learning algorithm to favour one solution over another.
There exists a large zoo of possible network layers, each with their own inductive biases.
The ML engineer must select, mix, and match these layers to create a network best suited to the task at hand.

One common guide is that the model should be well suited for the type of data it operates on and as in as in physics, symmetries are a good principle to follow.
Many layers are designed with equivariance or invariance in mind.
Convolutional layers are well suited for image or otherwise grid-like data as they are somewhat equivariant to translations.
Message passing networks work well for graph data as they are equivariant to the permutation of the nodes.
There exist many other layers designed to be equivariant to specific transformations on their inputs.
For example LorentzNet~\cite{LorentzNet} operates on particle physics data and is designed to be equivariant to rotations or shifts in reference frame.
Another rough philosophy is that individual layers should be simple, and easy to implement, as it is their composition and overparameterization that gives the model its expressive power.

Inductive biases do not have to be explicitly defined.
For example, fitting a linear regression model to data assumes explicitly that the relationship between the input and output is linear.
Doing so using the least-squares loss function implicitly assumes that the linear relationship is corrupted by Gaussian noise with constant variance.
For deep learning, examples of inductive biases include the choice of loss function, regularization, the optimization algorithm, and even the model architecture itself.
Weight decay can be seen as an inductive bias towards simpler models, as prioritizes solutions with small parameters.
Ideal inductive biases should improve the performance of the model primarily by aiding the optimization process.
However, too restrictive biases can limit model performance by excluding valid solutions.
Inductive biases can also be understood in terms of the bias-variance trade-off, where flexibility is traded for generalization.

\subsection{The Multi-Layer Perceptron}

The Multi-Layer Perceptron (MLP) is arguably the simplest form of a deep neural network.
It is also referred to as a dense network, fully connected network, or sometimes confusingly as a feed-forward neural network (FFN)\footnote{Which is usually used to describe any network without loops in the computational graph.}
In an MLP, there is a single input and output, both of which are rank-1 tensors but can have differing numbers of features.
A minimal working example of an MLP is a series of affine transformations\footnote{Often called a linear layer, but the operation almost always includes a bias term.}, each parameterized by a weight matrix $W$ and bias vector $\mathbf{b}$, followed by a non-linear activation function $\sigma$.
The depth of the MLP is the number of intermediate representations in the computational graph between the input and output.
A minimal MLP with two hidden layers can be written as
\begin{align}
    \mathbf{a}_0 &= \mathbf{x}, \\
    \mathbf{a}_1 &= \sigma_0(W_0 \mathbf{a}_0 + \mathbf{b}_0), \\
    \mathbf{a}_2 &= \sigma_1(W_1 \mathbf{a}_1 + \mathbf{b}_1), \\
    \mathbf{a}_3 = \mathbf{\hat{y}} &= \sigma_2(W_2 \mathbf{a}_2 + \mathbf{b}_2),
\end{align}
where $\mathbf{x}$ is the input, $\mathbf{\hat{y}}$ is the output, and $\mathbf{a}_i$ are the intermediate representations (or activations) of the network.
The parameters of this network are is the full set of weights and biases from each layer, $\theta = \{W_0, \mathbf{b}_0, W_1, \mathbf{b}_1, W_2, \mathbf{b}_2\}$.

Activation functions exist to introduce non-linearities in the computational graph and are applied element-wise to the tensors.
As the composition of affine transformations is itself an affine transformation, without activation functions, the entire MLP could be reparameterized as a single affine transformation, no matter its depth, greatly hindering its expressive power.
With these induced non-linearities, MLPs can approximate any continuous function~\cite{ApproximationSuperpositionsSigmoidal, ApproximatingContinuousFunctions, UniversalApproximationDeep}, given enough hidden units and the right activation functions.

The activation function in the output layer is typically chosen to match the expected range of the target variable.
Historically, the sigmoid function was used as the primary activation in the hidden layers, due to its analogy with biological activation.
However, it can be shown that even the simplest function that breaks linearity is sufficient, and the ReLU~\cite{ReLU} function $\text{ReLU}(x) = \max(0, x)$ has become the popular choice for hidden layers in modern networks.
Other common choices for hidden layer activations are shown in \Cref{fig:activations}.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/transformers/activations.pdf}
    \caption{Common activation functions used in deep learning.}
    \label{fig:activations}
\end{figure}

\section{Training with Gradient Descent}

Training a deep neural network is an optimization problem where the best parameters $\theta^*$ of the network are those that minimize the combined loss $\mathcal{L}$ over the training set $\mathcal{D}$.
\begin{equation}
    \theta^* = \argmin_{\theta} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(\theta, x_i).
\end{equation}
Here the loss function includes some empirical risk calculation and the sample $x \in \mathcal{D}$ is used as shorthand to denote the full sample from the training set, including the target if present.
Due to the size of the parameter space and the non-linearity of the model, the $\theta^*$ can not be found analytically.
The most common method for training deep neural networks is mini-batch stochastic gradient descent (SGD) or one of its many variants~\cite{Perceptron}.
SGD is an iterative optimization algorithm that attempts to minimize a loss function by adjusting the parameters of the model in the direction of the negative gradient of the loss.
The training set is partitioned into batches of size $B$ which are iterated through.
At each iteration, the loss for the batch is calculated, and the parameters are updated using gradient descent with some learning rate $\eta$,
\begin{equation}
    \theta \leftarrow \theta - \eta \nabla_{\theta} \frac{1}{B} \sum_{i=1}^B \mathcal{L}(\theta, x_i).
\end{equation}
Both the $B$ and the $\eta$ are hyperparameters of the training algorithm.
Once the entire training set has been processed, this is considered one epoch.
Training continues until some stopping criterion is met, such as a saturation of the loss.
Typically, models are trained for many epochs and the training set is shuffled between each epoch to prevent correlated updates which may hinder convergence.
Ideally we would have $B=N$, this is called batch gradient descent.
However, this is often infeasible due to memory constraints.
So instead we use a noisy estimate of the gradient, by using a subset of the training set.
True SGD uses a batch size of $B=1$.

The learning rate is a crucial hyperparameter of the training algorithm, however it is common to modify it during training.
Many large networks require a warm-up phase~\cite{SGDRStochasticGradient} where the learning rate is slowly increased from zero to its maximal.
Without this step, large models like transformers are prone to diverging early in training.
The learning rate is also decayed towards the end of training helping the network converge to a local minimum, avoiding oscillation improving the learning of complex patterns~\cite{HowDoesLearning}.
Often these phases are repeated~\cite{CyclicalLearningRates} or performed once~\cite{SuperConvergenceVeryFast}, ending the training after the first cycle.

There exist many extensions to this basic algorithm that attempt to improve robustness and convergence.
One common method is the accumulation of a moving average of the gradients, analogous to momentum in physics~\cite{Momentum}.
This assists in traversing regions of the parameter space where curvature is higher in one direction than another.
Another common method is the use of adaptive learning rates, where the per parameter learning rate is adjusted based on the fluctuations in the gradient~\cite{Adagrad, RMSProp}.
\Cref{alg:adam} shows the update rule for one of the most common variants, the Adam optimizer~\cite{Adam}, which combines both momentum and adaptive learning rates.
It introduces two hyperparameters $\beta_1$ and $\beta_2$ which control the exponential decay rates of the moving averages.

\begin{algorithm}
\caption{The Adam optimizer, where $\mathcal{L}_B(\theta)$ is the loss calculated over a mini-batch, $\eta$, $\beta_1$, $\beta_2$ are hyperparameters, $t$ is the current iteration and $\epsilon$ is a small constant to prevent division by zero.}
\label{alg:adam}
\begin{algorithmic}[1]
\State $\mathbf{m} \gets 0$ \Comment{Initialize first moment vector}
\State $\mathbf{s} \gets 0$ \Comment{Initialize second moment vector}
\For{each mini-batch}
    \State $\mathbf{g} \gets \nabla \mathcal{L}_B(\theta)$ \Comment{Compute gradients}
    \State $\mathbf{m} \gets \beta_1 \mathbf{m} + (1 - \beta_1) \mathbf{g}$ \Comment{Update biased first moment estimate}
    \State $\mathbf{s} \gets \beta_2 \mathbf{s} + (1 - \beta_2) \mathbf{g} \otimes \mathbf{g}$ \Comment{Update biased second moment estimate}
    \State $\hat{\mathbf{m}} \gets \frac{\mathbf{m}}{1 - \beta_1^t}$ \Comment{Correct bias in first moment}
    \State $\hat{\mathbf{s}} \gets \frac{\mathbf{s}}{1 - \beta_2^t}$ \Comment{Correct bias in second moment}
    \State $\theta \gets \theta - \eta \frac{\hat{\mathbf{m}}}{\sqrt{\hat{\mathbf{s}} + \epsilon}}$ \Comment{Update parameters}
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Normalization}

Training via gradient descent requires the gradient with respect to every parameter in the model.
This leads to a balancing problem with the vanishing and exploding gradient problems manifesting as the gradients become too small or too large to be useful for training.
This is especially problematic in deep networks, where the gradients must be propagated through many layers, requiring the repeated application of the chain rule.
As the gradients are linked to the scale of the intermediate tensor representations in each layer, it is good practice to ensure that these are roughly the same scale.
It is common to target activations with a mean of zero and a standard deviation of one by including normalization layers.

Another reason for this is that most activation non-linearities are near zero, \Cref{fig:activations}.
Oversaturating these functions ensure that the layer becomes linear, and the network expressivity decreases.

Normalization starts with the input data, which is often normalized in a preprocessing step and typically is performed using the statistics of the training set.
Variables with long tails are typically transformed to have a more Gaussian distribution, such as by taking the logarithm.
Within the network itself, normalization layers $L_{\text{scale}}$ are used to ensure that the activations of each layer have a mean of zero and a standard deviation of one.
Methods such as batch normalization~\cite{BatchNorm} or layer normalization~\cite{LayerNorm} are widely used and both have been shown to be greatly effective, with LayerNorm seen as essential for transformer neural networks~\cite{AttentionIsAllYouNeed}.

Another crucial feature to stabilise the gradients is to manually clip them before applying the update.
This can be done based on some maximal value, but is more commonly done by scaling the gradient such that their combined norm does not exceed some threshold~\cite{WhyGradientClipping}.

\subsection{Residual Connections}

Residual connections are another simple and effective technique for training deep networks.
Here the learnable transformation of the layer is added to the input, rather than replacing it,
\begin{equation}
    \mathbf{a}_{i+1} = \mathbf{a}_i + f(\mathbf{a}_i),
\end{equation}
where $f$ is the transformation of the layer.
A single operation of this form is called a residual block.
Residual connections have been shown to improve the training of very deep networks, as they allow the gradient to flow directly through the network, bypassing the squashing or exploding of the gradient in the intermediate layers.
Residual connections are a key component of almost all moder arcitectures from convolutional neural networks~\cite{ResNet} to transformers~\cite{Attention}.
The residual connection is usually additive, but other operations exist, for example UNets~\cite{UNet} use concatenation.
If the input and output of the layer have different dimensions, the residual path is adjusted by a minimal transformation, in a convolutional network this is typically a $1 \times 1$ convolution.

The addition of two signal paths can lead to a doubling of variance.
This can cause the network to become unstable, especially in the early stages of training.
One method to circumvent this is the inclusion of a normalization layer after the residual block, as was done in the original transformer architecture~\cite{Attention}, and is referred to as post-normalisation.
However, this was shown to be detrimental to very deep networks and recent methods use pre-normalisation~\cite{PreLN} (PreNorm), where the input to the learnable layer is normalized.
With a PreNorm residual block one can prevent an exploding variance by scaling the residual path by a small constant, such as $\frac{1}{\sqrt{2}}$~\cite{StyleGAN2}.
Recent methods allow this constant to be trainable with some set it to zero at the beginning of training such that the block behaves like an identity function~\cite{ReZero, SkipInit, Fixup}.
This was extended to the LayerScale method~\cite{GoingDeeper}, where a full learnable tensor $\mathbf{L}_s$ matching the output shape of the layer is used to scale the residual path.
It too is initialised to zero, and the network is trained to learn the optimal scaling.
A PreNorm residual block with LayerScale follows,
\begin{equation}
    \mathbf{a}_{i+1} = \mathbf{a}_i + \mathbf{L}_\text{scale} \otimes f( L_{\text{norm}}(\mathbf{a}_i) ),
\end{equation}
and such a large models like transformers.

\section{Generalization}

Generalizaion in the context of ML refers to the ability of a model to perform well on new, unseen data.
This is in most cases the primary goal of training such a model.
As neural networks are highly flexible, it is possible that they can ``memorize'' the training set, achieving a very low loss but failing to generalize to new data.
This phenomenon is called overfitting, and it is a common problem in deep learning.

Detecting overfitting can be done through cross-validation, where the model is trained on a subset of the training data and tested on the rest.
The most simple case of this is to split the available data into a training set and a validation set.
Gradient descent is performed on the training set, but the loss is also calculated over the validation set at regular intervals.
The difference in performance between the training and validation set is a good indicator of overfitting.
Often the model chosen for future applications is the one that performs best on the validation set.

Regularization methods like weight decay, dropout, and drop-path are effective in reducing overfitting by penalizing complex models.
Weight decay places a penalty on the size of the parameters in the network, pushing it towards simpler solutions.
Dropout is a technique where a random subset of the activations in the network are set to zero during training, effectively training an ensemble of models.
Another interpretation is that droupout simply injects
Drop path and stochastic depth is an extension of dropout where entire layers are removed from the network during training.
Data augmentation can also help by artificially expanding the training dataset.
Confusingly, increasing the size of the network can also help prevent overfitting.
This is known as the double descent phenomenon~\cite{UnderstandingDoubleDescent}, whereas the effective model capacity grows, more solutions become accessible, and the model becomes a self ensemble.

Domain shift similar to overfitting, but occurs when the distribution of the training data differs from the distribution of the test data.
This is a significant issue in fields HEP, where models are trained on simulated samples, in order to have access to ground truth labels, but must be applied to real data.
Domain adaptation and adversarial training are methods to address this.

Finally, unwanted biases in the training set can lead to skewed predictions.
The distribution of the target variable acts as a prior for the model, and if this distribution is not representative of the population, the model will make biased predictions.
Reweighting or resampling the training set can help mitigate this.


\chapter{Graph Neural Networks}

% The following discussion on graph neural networks is primarily adapted from \textcite{GraphIntro,RelationalInductiveBiases,GraphRepresentationLearning}.

The philosophy behind graph neural networks (GNNs) is that nature is best understood when broken down into compositional elements.
Understanding these elements and the rules that define their relationships is key to understanding the system as a whole.
This reductionist approach is used by much of the scientific community, and indeed is the basis for all of particle physics.
Graphs are an example of explicitly structured data containing entities and pairwise relations between them.
Relational reasoning is the development of rules or functions that describe how the properties of entities modify the relations between them, and how the relations modify the properties of the entities.
GNNs are a class of deep learning models that operate on graph-structured data and employ relational reasoning complete tasks by directly manipulating the rules, entities, and relations of a system.

An example of successful relational reasoning being used in deep learning outside GNNs is in image processing tasks.
An image is typically represented as a tensor of shape $(H, W, C)$, where $H$ is the height, $W$ is the width, and $C$ is the number color channels, using the pixel values as the tensor elements.
Image processing tasks were once the domain of MLPs \citetemp{MLPforMNIST}.
This meant that the image must first be flattened to a tensor of shape $H \times W \times C$, and then passed through a series of fully connected layers, whereby every element of the input tensor is connected to every element of the output tensor.
While this leads to a flexible model, it also destroys the spatial structure of the image and no reusable relationships or rules are discovered.
Instead, we would like to use a model that preserves the structure of the data.

The building block of the convolutional neural network is a learnable convolutional kernel.
The kernel scans over the input tensor, computing the weighted sum of the elements in the kernel window.
This introduces two key relational inductive biases: locality and translation invariance.
Locality refers to the assumption that the relationship between two elements is stronger if they are close together in the input space.
Translation invariance refers to the assumption that the relationship between two elements is the same regardless of their absolute position in the input space.
Locality is enforced by using a kernel width of limited size and translation invariance is enforced by reusing the same kernel across the input space.
Each convolutional kernel defines a learned and reused rule for how elements within the structure of the input tensor are related.

This is an example of geometric deep learning, also called a relational inductive bias, whereby the known structure of the data is embedding into the architecture of the model itself, rather than being learned directly from the data.
This reduces the number of parameters required in the model, allowing for more statistically efficient learning.

\section{Defining a Graph}

A graph $\mathcal{G}$ is a data structure that consists of a set of $N$ attributed nodes $\mathcal{N} = \{\mathbf{x}_i\}_{i=1}^{N}$ interconnected by a set of $N^e$ attributed edges $\mathcal{E} = \{(\mathbf{e}_{k}, r_k, s_k)\}_{k}^{N^e}$.
Here $\mathbf{e}_{k}$ are the edge attributes, $s_k$ is the index of the sender node and $r_k$ is the index of the receiver node.
The graph is directed and self-loops are allowed.
It is also useful in many contexts to define a global attribute $\mathbf{u}$ which describes the entire graph as a whole.
A singular-graph can be represented as a tuple $\mathcal{G} = (\mathcal{N}, \mathcal{E}, \mathbf{u})$.
The structure of the graph is only defined by the interconnections between the nodes, the actual ordering of the nodes in the set is arbitrary.
Therefore, any operation on the graph should not depend on the ordering of the nodes.
Not that the adjacency matrix of the graph is not explicitly defined, but can be inferred from the elements of $\mathcal{E}$.

This representation of a graph is very general and each of the terms $\mathcal{N}$, $\mathcal{E}$, and $\mathbf{u}$ are optionally included.
For example, set data is a special case of a graph where there are no edges connecting the nodes.
Or we could allow multiple sets of edges to exist between nodes $\{\mathcal{E}^1, \mathcal{E}^2, \ldots\}$, defining a multi-graph.
An undirected graph can be represented using a fully symmetrical edge set.

Each of the edge $\mathbf{x}_i$, node $\mathbf{e}_{k}$, and global $\mathbf{u}$ properties of the graph can be encoded as any type of data, even graphs themselves.
However, in most use cases dealt with in deep learning they are real valued tensors.
It is also common for all the nodes to share the same dimensionality, and the same for the edges, imposing an equivalence between the elements of the set.
These are not strict requirements, but it simplifies the design of the model as it allows operations on the individual elements to be parameterized with standard deep learning building blocks, such as linear layers, MLPs, and CNNs.

Representing a graph in code required tuple or dictionary of real valued tensors $(\mathbf{X}, \mathbf{E}, \mathbf{U})$.
Here, $\mathbf{X} \in \mathbb{R}^{N \times d_x}$, $\mathbf{E} \in \mathbb{R}^{N \times N \times d_e}$, and $\mathbf{U} \in \mathbb{R}^{1 \times d_u}$, where $d_x$, $d_e$, and $d_u$ are the dimensionality of the node, edge, and global features respectively.
The edge matrix is typically ordered such that $\mathbf{E}_{ij}$ is the edge from node $i$ to node $j$.
To represent the existence or absence of an edge between two nodes, a binary adjacency matrix $\mathbf{A} \in \{0, 1\}^{N \times N}$ can be used where $\sum{\mathbf{A}} = N^e$.
Alternatively, one could use a sparse tensor representation, which is more memory efficient but can complicate the implementation of the model.

Almost all GNNs operate on the graph by facilitating message passing between the nodes.
In this framework the nodes send messages to their neighbours.
These messages are then aggregates and used to update the node's attributes.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/transformers/graphs.pdf}
    \caption{A directed multi-graph with node, edges a global attribute $\mathbf{u}$. Highlighted is the edge $\mathbf{e}_{k}$ which connects the sender node $s_k$ to the receiver node $r_k$. Adapted from \textcite{RelationalInductiveBiases}.}
    \label{fig:graph}
\end{figure}

\section{Graph Type Data}

Graph or set representation is fairly ubiquitous in the real world and can be used for anything that can be broken down into discrete elements.
Some examples are shown in \Cref{fig:graph_examples}.
Relevant to this thesis, particle interactions may be expressed as sets, where the stable particles are the nodes.
This is natural representation for particle data, which has no inherent ordering.
Complete interactions and decay chains, like all-hadronic $ttH$ production process shown in \Cref{fig:feynman}, can be represented as a graph.
In this instance, it might be more natural to represent the virtual particles in the Feynman diagram as edges rather than the nodes.
Chemical compounds can be easily represented as graphs, where the atoms and the chemical bonds between them are the nodes and edges respectively.
\Cref{fig:chemical} shows the structure of the antibiotic rifampicin.

Text can be converted into a graph by treating words or subwords as nodes~\citetemp{AttentionIsAllYouNeed}.
Though graphs typically have no natural ordering, it can be imposed by appending the order of the element in the sequence to the attributes of each node.
The edges may also be connected such that they respect the order of the sequence, as in \Cref{fig:text}, where information can only flow forward in the sequence.
This has significant consequences for many generative modelling tasks, essentially providing a natural way to train and perform autoregressive sampling.

Even data which is not inherently graph-like can be broken down into constituent parts and interconnected as desired to form a graph.
Here, the choice of representation is crucial to the rules learned by the model.
As information can only propagate along defined edges in the graph, the existence of an edge implies a direct relationship between the sender and receiver nodes.
An image may be patched into a grid where each patch is treated as a node as shown in \Cref{fig:dog}.
Like the CNN, locality can be imposed by only connecting patches close to each other.
A patched image may not have any edge attributes at all, alternatively, translationally invariant information and could be encoded in the edge attributes, such as the vertical and horizontal distance between patches.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Feynman/tth.pdf}
        \caption{Particle interactions}
        \label{fig:feynman}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/Rifampicin_structure.pdf}
        \caption{Chemical compounds}
        \label{fig:chemical}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/patched_image.png}
        \caption{Images (patched)}
        \label{fig:dog}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{Figures/transformers/text.pdf}
        \caption{Language (tokenized)}
        \label{fig:text}
    \end{subfigure}
    \caption{Examples of data that can be represented as graphs.}
    \label{fig:graph_examples}
\end{figure}

\subsection{Interconnectivity}

When constructing a graph representation of a data sample, it is crucial to consider its interconnectivity.
The interconnectivity of the graph is closely tied to two key issues in grpah neural networks: the over-squashing problem and the over-smoothing problem.

While locality is often a useful inductive bias, limiting the connections of each node to its local neighbourhood, it can also mean that long term dependencies may get lost.
This is because multiple rounds of message passing are required to propagate information between distant nodes as shown in \Cref{fig:neighbours}.
On each node is the number of message passing steps required to propagate information to the node $\mathbf{x}_i$.
With each round, exponentially more neighbouring nodes need to project information into a fixed-size vectors.
This phenomenon is known as the over-squashing problem, and is exacerbated by bottlenecks in the graph structure as shown in \Cref{fig:bottleneck}, where the information between nodes $\mathbf{x}_i$ and $\mathbf{x}_j$ must pass along a single edge which must also carry information from their respective neighbourhoods.

There are many proposed solutions to the over-squashing problem.
The inclusion of global features in the graph may help model long range dependencies, however the size of the global feature would need to scale with the cardinality of the set.

One solution is to increase the number of edges in the graph, allowing for more direct paths between nodes.
This procedure can be focussed around bottlenecks in the graph, identified by regions of low Forman curvature~\citetemp{Bottlenecks}.
The extreme case of this is a fully connected graph, where edges exist between all pairs of nodes in both directions.
Presenting the data as a fully connected graph allows the model to choose for itself which interactions are important, which it can do if equipped with an attention mechanism~\citetemp{AttentionIsAllYouNeed}.
Attention refers to the process of weighting the incoming messages based before aggregation.
It allows the node to ``pay more attention'' to certain neighbours.
This introduces more expressivity into the model by distinguishing the layers for deciding the content of a message and the layers for deciding its importance.

Using a fully connected graph like this would mean sacrificing the relational inductive bias of locality.
Counterintuitively, this can be beneficial in many cases, for example the most successful models on images use fully connected graphs~\citetemp{VisionTransformers} despite dropping the locality inductive bias of the convolutional kernel.
This is because the locality in images is not always useful, for example if an object in the image is obscured and only partially visible physically separated patches.
In the example of the chemical compound in \Cref{fig:chemical}, the edges are defined by the chemical bonds between the atoms.
However, long range dependencies do exist in chemistry.
The function of one part of a protein may be altered by the presence of a ligand on the other side of the molecule.
Constraining the graph on which the messages are passed to the physical structure of the data may limit the model from learning these relationships.

This highlights a conflict between designing a model strong or minimal inductive biases and whether the computational graph and the input data structure should be tied together at all.
Models such as the perceiver~\citetemp{Perceiver} project input information onto a completly different latent graph and run the message passing on that.
Recent work in vision transformers have shown that the addition of extra, fully learnable nodes to the graph can improve performance~\citetemp{Registers}.

However, there are some downsides to simply increasing the connectivity of the graph.
First, this can be computationally prohibitive when the cardinality of the set is large, as the number of edges grows quadratically.
Many models are looking to find way to minimize the interconnectivity of the graph while preserving model quality~\citetemp{SparseAttention}.
Second, it exacerbates the other problem plaguing GNNs, the over-smoothing problem.
Over-smoothing refers to the phenomenon where the nodes in the graph become homogenous, containing the same information, after many rounds of message passing.
This is because with each round of message passing, the information from the neighbouring nodes is averaged, and the information from the original node is diluted.
This one of the reasons that the depth of most GNNs is notably shallower than other deep learning models.
There are many proposed solutions to this issue, such as residual connections and normalization layers.
Attention is also a viable method to combat over-smoothing.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/neighbours.pdf}
        \caption{}
        \label{fig:neighbours}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/bottleneck.pdf}
        \caption{}
        \label{fig:bottleneck}
    \end{subfigure}
    \caption{\subref{fig:neighbours} The locality of information flow in a graph. \subref{fig:bottleneck} A graph with a bottleneck.}
    \label{fig:over_squashing}
\end{figure}

\subsection{Graph Tasks}

In analysing graph data within real-world contexts, there exists three primary types of predictive tasks: graph-level, node-level, and edge-level predictions.
For graph-level tasks, the aim is to predict an attribute or property that encompasses the entire system as a whole.
An example of this in the context of computer vision would be the classification of the entire image~\citetemp{VisionTransformers}.
Node-level tasks predict the properties associated with each individual node.
They are somewhat analogous to image segmentation.
Finally, there are edge-level tasks which predict  properties or even the existence of edges between the nodes.
An example of this may be predicting the type of virtual particle in a Feynman diagram, \Cref{fig:feynman}.

\section{The Graph Network Block}
\label{sec:gn_block}

The Graph Network Block (GN-Block) is a useful building block for constructing deep learning models that operate on graph-structured data.
It offers a framework that generalizes to many other existing graph-based models, such as graph convolutional networks (GCNs), graph attention networks (GATs), deep sets, and even transformers.
A GN-Block is defined as a series of operations that update the node, edge, and global attributes via facilitating the exchange and aggregation of information according to the structure of the graph.
The GN-Block does not change the cardinality of the features present, only updating their attributes,
\begin{equation}
    \text{GN}(\mathcal{N}, \mathcal{E}, \mathbf{u}) = (\mathcal{N}', \mathcal{E}', \mathbf{u}')
\end{equation}
The GN block is composed of three sub-blocks involving three update functions, $\phi^v$, $\phi^e$, and $\phi^u$, and three aggregation functions, $\rho^{e \to v}$, $\rho^{e \to u}$, and $\rho^{v \to u}$.
Given an input graph $\mathcal{G} = (\mathcal{N}, \mathcal{E}, \mathbf{u})$ the GN-Block proceeds as follows.

\begin{enumerate}
    \item \textbf{Edge Block:} The edges are updated using the current edge attributes, the attributes of the sender and receiver nodes, and the global attribute.
    \begin{equation}
        \mathbf{e}_k' = \phi^e(\mathbf{e}_k, \mathbf{x}_{s_k}, \mathbf{x}_{r_k}, \mathbf{u}).
    \end{equation}
    All edges throughout the graph are updated in this manor which can be executed in parallel.
    The updated edges can be thought of as the message being passed from the sender to the receiver.
    \item \textbf{Node Block:} Incoming edge information is aggregated for each node.
    For node with index $i$, this is represented by the set $E_i$.
    \begin{equation}
        \mathbf{\bar{e}}_i = \rho^{e \to v}(E_i) = \rho^{e \to v}\left(\{(\mathbf{e}_k', r_k, s_k) | r_k = i\}\right)
    \end{equation}
    All nodes are then updated using its current attributes, the aggregated edge information, and the global attribute.
    Updating the nodes using incoming information is called message passing.
    \begin{equation}
        \mathbf{x}_i' = \phi^v(\mathbf{\bar{e}}_i, \mathbf{x}_i, \mathbf{u}).
    \end{equation}
    \item \textbf{Global Block:} All edge information and node information across the graph is aggregated,
    \begin{alignat}{2}
        \mathbf{\bar{v}} &= \rho^{v \to u}(\mathcal{N}') &&= \rho^{v \to u}(\{\mathbf{x}_i\}_{i=1}^{N}) \\
        \mathbf{\bar{e}} &= \rho^{e \to u}(\mathcal{E}') &&= \rho^{e \to u}(\{\mathbf{e}_k'\}_{k=1}^{N^e}),
    \end{alignat}
    and used to update the global attribute,
    \begin{equation}
        \mathbf{u}' = \phi^u( \mathbf{\bar{e}}, \mathbf{\bar{v}}, \mathbf{u}).
    \end{equation}
\end{enumerate}

The GN-Block describes a basic graph to graph transformation which can be composed to form a deeper model.
To make the block learnable, each of the update or aggregation functions may be parameterized by a separate neural network.
The only requirement for any of the aggregation functions is that they are permutation invariant and may accept a variable number of arguments.
This structure also lends itself to the different types of tasks performed on graph type data.
For example, the final GN-Block in the stack may only contain an edge block for an edge-level task.

It is important to investigate exactly what inductive biases are being imposed by the GN-Block.
First, it is clear that the updates to the graph do not depend on the order of the nodes.
Formally, the node and edge updates $(\mathcal{N}, \mathcal{E}) \rightarrow (\mathcal{N}', \mathcal{E}')$ are permutation equivariant to the node set, and the global update $U \rightarrow U'$ is permutation invariant.
The GN-Block applies the same update and aggregation functions to all nodes and edges in the graph.
This reflects the relational inductive bias which seeks to learn general rules that apply to all entities in the system while also adhering to the equivalence of the elements within the set.
Furthermore, as the GN-Block only prescribes the rules for how the features of a graph are updated, it is agnostic to the structure and size of the input graph.
This allows the same model to be applied to input sets with varying cardinality.
This is a crucial feature for many applications, such as particle physics, where the number of particles in an event can vary.
Finally, information flow between elements of the set can follow two paths.
Either, the information propagates along the edges of the graph, enforcing locality, or it is aggregated into the global attribute, then redistributed to the nodes.
This allows for the model to learn both local and long-range interactions.

\section{Different Graph Networks}

A GNN is a general framework for constructing deep learning models that operate on graph-structured data.
Almost all GNNs facilitate message passing and layers within the network can be thought of as some specialization of the GN-Block, depending on the forms of the $\phi$ and $\rho$ functions, and the existence of edge, node, and global features.
The specifics of these functions mean that there are many GNN variants.

A full GN-Block is shown in \Cref{fig:full_gn_block}.
A basic implementation, where all graph attributes are tensors, can be done using MLPs for each of the update functions, and summation for the aggregation functions.
This creates the following update shown in \Cref{alg:gn_block}.

\begin{algorithm}
    \caption{Basic GN-Block using MLPs}
    \label{alg:gn_block}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Graph attributes $\mathcal{G} = (\mathcal{N}, \mathcal{E}, \mathbf{u})$
        \State \textbf{Output:} Updated graph attributes $\mathcal{G}' = (\mathcal{N}', \mathcal{E}', \mathbf{u}')$
        \For{each edge $\mathbf{e}_k$ in $\mathcal{E}$}
            \State $\mathbf{e}_k' \gets \text{MLP}_e([\mathbf{e}_k, \mathbf{x}_{s_k}, \mathbf{x}_{r_k}, \mathbf{u}])$ \Comment{update edge features}
        \EndFor
        \For{each node $\mathbf{x}_i$ in $\mathcal{N}$}
            \State $\mathbf{\bar{e}}_i \gets \sum_{k: r_k = i} \mathbf{e}_k'$ \Comment{aggregate incoming information per node}
            \State $\mathbf{x}_i' \gets \text{MLP}_v([\mathbf{\bar{e}}_i, \mathbf{x}_i, \mathbf{u}])$ \Comment{update node features}
        \EndFor
        \State $\mathbf{\bar{v}} \gets \sum_{i} \mathbf{x}_i'$ \Comment{aggregate features across the graph}
        \State $\mathbf{\bar{e}} \gets \sum_{k} \mathbf{e}_k'$
        \State $\mathbf{u}' \gets \text{MLP}_u([\mathbf{\bar{e}}, \mathbf{\bar{v}}, \mathbf{u}])$ \Comment{update global features}
    \end{algorithmic}
\end{algorithm}

where the square brackets denote tensor concatenation.
This provides a simple model that can be trained end-to-end using gradient descent.
Other GNN layers can be framed similarly.
Even the deep set, which has no pairwise interactions between the nodes can be seen as a reduced form of a GN-Block, as shown in \Cref{fig:deep_set}.
Here the operations are simply,
\begin{align}
    \mathbf{x}_i' &= \text{MLP}_v\left(\left[\mathbf{x}_i, \mathbf{u}\right]\right),\\
    \mathbf{u}' &= \text{MLP}_u\left(\left[\sum_{i} \mathbf{x}_i', \mathbf{u}\right]\right).
\end{align}

GN-Blocks which preserve the existence of graph features, unlike the deep set which has no node features in the output, can be stacked together to form a deep GNN.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/full_gn.pdf}
        \caption{Full GN Block}
        \label{fig:full_gn_block}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/deepset.pdf}
        \caption{Deep Set}
        \label{fig:deep_set}
    \end{subfigure}
    \caption{Examples of different GN-Block implementations \subref{fig:full_gn_block} A full GN-Block has persistent edge and global features. \subref{fig:deep_set} A deep set only applies pooling to the node features to produce a global feature.}
\end{figure}

\section{Transformers are Graph Networks}
\label{sec:transformers}

It is hard to overstate the impact of the transformer model~\citetemp{AttentionIsAllYouNeed} on the field of deep learning.
Originally designed for sequence-to-sequence tasks, transformers grew to dominate all NLP benchmarks.
They marked such a significant improvement that research in other sequence-based models, such as RNNs, almost ceased entirely.
In 2019, Google announced that they were using the transformer based BERT model~\citetemp{BERT} to power their search engine~\citetemp{GoogleBERT}.
They would also switch to using transformers for their translation service~\citetemp{GoogleTranslate}.
Starting in 2018, OpenAI began releasing the GPT~\citetemp{GPT} series of transformer models, arguably triggering a boom around large language models that continues to this day.
In 2020, transformers were adapted to work on image data~\citetemp{VisionTransformers} and have since become the state-of-the-art model for most computer vision tasks, a task that was previously dominated by CNNs, as shown in \Cref{fig:benchmarks}.
Even image generation tasks which one used UNets are now being performed by transformers~\citetemp{DIT, SD3}.
Key to the transformer success is how simple they are to implement and how well the performance seems to scale with the size of the model.
Transformers have become so ubiquitous that they are now considered the default choice for many deep learning tasks, regardless of the data type.

At its core, the vanilla transformer encoder is simply another form of a GNN\@.
There are many variants for the transformer model, but they all share the same basic features.
Contrary to popular belief, these models do not natively run on sequences but on sets, and must be adapted to work on data with an inherent order.
We will focus on two main types of transformers, the transformer encoder and the transformer decoder.
Specifically, the transformer encoder can be seen as a non-local neural network (NLNN)~\citetemp{NLNN} as shown in \Cref{fig:transformer}.
This function operates on a pure set, with no persistent edge or global features.
It involves and message passing step, done using multi-headed attention, followed by a node update step using an MLP.
They also employ residual connections and many forms of normalization to stabilize training and prevent over-smoothing.

The jargon of transformers differs slightly from that of GNNs.
Graphs and sets contain nodes, which are called tokens in many papers on transformers.
The term attention is also used more broadly in transformers, not just referring to the means of weighting an aggregation, it is sometimes used to refer to the entire message passing step.
Furthermore, saying that a node attends to another is equivalent to saying that the node receives a message from the other.
People
The adjacency matrix in a graph is also called the attention mask in a transformer.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/imagenet.pdf}
        \caption{}
        \label{fig:imagenet}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/transformers/coco.pdf}
        \caption{}
        \label{fig:coco}
    \end{subfigure}
    \caption{Evolution of the state-of-the-art models in two common computer vision benchmarks~\cite{paperswithcode}. The size of the markers corresponds to the number of trainable parameters in the model. \subref{fig:imagenet} Accuracy on the ImageNet classification benchmark, \subref{fig:coco} Bounding box mean average precision (BOX MAP) the COCO object detection benchmark.}
    \label{fig:benchmarks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{Figures/transformers/nonlocal.pdf}
    \caption{NLNN / Transformer Encoder}
    \label{fig:transformer}
\end{figure}

\subsection{Attention is Message Passing}
\label{sec:attention}

There are hundreds of ways to parameterize the message passing step in a GNN, and the one used in the transformer is the scaled dot-product attention (SDPA).

In SDPA, the message passed between nodes is a linear projection of the sender node attributes.
This means that each node sends out the same message to all of its neighbours.
This sent information is referred to as the value of the node, where is a learnable weight matrix.
The attention weight is based on a dot product between separate projections of the sender and receiver nodes, called the key and query respectively.
A softmax function is applied to the attention weights to turn the aggregation operation into a weighted mean of the incoming messages.
To prevent the gradients from exploding, the attention logits are scaled by the square root of the dimensionality of the key and value tensors $d_k$.

In the framing of the GN-Block, self-attention has the edge update is factorized into a tuple containing a scalar expressing the message weight $a_k'(\mathbf{x}_{s_k}, \mathbf{x}_{r_k})$ and a vector function for calculating the message content $\mathbf{v}_k'(\mathbf{x}_{s_k})$.
Only $\alpha^e$ depends on the pairwise interaction of both the sender and receiver nodes.
The pooling operation $\rho^{e \to v}$ normalizes over the receiver weights before aggregating the messages.
The block can be written as,
\begin{equation}
    \begin{aligned}
        \mathbf{e}_k' &= \phi^e(\mathbf{e}_k, \mathbf{x}_{s_k}, \mathbf{x}_{r_k}, \mathbf{u}) \\
        &= ( a_k', \mathbf{v}_k' ) \\
        &= \left( \exp \left( \frac{1}{d_k} \mathbf{W}^q \mathbf{x}_{s_k} \cdot \mathbf{W}^k \mathbf{x}_{r_k} \right), \mathbf{W}^v \mathbf{x}_{s_k} \right), \\
        \mathbf{\bar{e}}_i &= \rho^{e \to v}(E_i) \\
       &= \frac{\sum_{k: r_k = i} a_k' \mathbf{v}_k'}{\sum_{k: r_k = i} a_k'}.
    \end{aligned}
\end{equation}
for learnable weight matrices $\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$, $\mathbf{W}^q \times \mathbb{R}^{d \times d_k}$, and $\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$.

This operation can be done efficiently for all nodes in parallel using basic matrix operations.
Furthermore, it allows us to extend to two forms, self-attention and cross-attention, both expressible by the following equation,
\begin{equation}
    \label{eq:attention}
    \text{Attention}(\mathbf{X_r}, \mathbf{X_s}) = \text{softmax}\left(\frac{\mathbf{X_s} \mathbf{W}^q (\mathbf{X_r} \mathbf{W}^k)^T}{d_k} + \mathbf{B}\right) \mathbf{X_s} \mathbf{W}^v.
\end{equation}
In this expression we distinguish between the tensor representing the sender nodes $\mathbf{X_s} \in \mathbb{R}^{N_s \times d}$ and the receiver nodes $\mathbf{X_r} \in \mathbb{R}^{N_r \times d}$.
Self-attention is where the sender and receiver set are the same $\mathbf{X_s} = \mathbf{X_r}$.
This is the standard message passing operation between nodes in a graph.
Cross-attention is where the sender set and receiver set is different $\mathbf{X_s} \neq \mathbf{X_r}$.
This operation is permutation invariant with respect to $\mathbf{X_s}$ and equivariant with respect to $\mathbf{X_r}$.
As shown by \Cref{fig:bipartite}, one can interpret this as the construction of a one-way bipartite graph between two sets, where all nodes in the sender set send messages to all nodes in the receiver set.
The output of the operation would therefore be the updates to the receiver nodes.
Note that the cardinality of the sender and receiver sets do not need to be the same.
Cross-attention is a powerful tool to condition one set on another and is used in many sequence to sequence tasks, from translation to image captioning and text to image generation.
Finally, the bias term $\mathbf{B}$ is a matrix of shape $(N_r, N_s)$ which is broadcasted to the shape of the attention logits.
This can be used to focus or ignore certain pairs of nodes.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/transformers/bipartite.pdf}
    \caption{(left) Self-attention is standard message passing between all nodes in a graph. (right) Cross-attention creates a one-way bipartite graph between two sets of nodes.}
    \label{fig:bipartite}
\end{figure}

The final major feature of a transformer's message passing step is multi-headed attention (MHA), shown in \Cref{fig:mha}.
This is where the operation is performed multiple time in parallel with different matrices $\mathbf{W}^q$, $\mathbf{W}^k$, $\mathbf{W}^v$, and $\mathbf{B}$.
The output of each head is concatenated and then mixed using a final linear layer.
\begin{equation}
    \begin{aligned}
    & \text{Head}_1 = \text{Attention}(\mathbf{X_r}, \mathbf{X_s}), \\
    & \ldots, \\
    & \text{Head}_H = \text{Attention}(\mathbf{X_r}, \mathbf{X_s}), \\
    \vphantom{x} \\
    & \text{MHA}(\mathbf{X_r}, \mathbf{X_s}) = \text{Concat}(\text{Head}_1, \ldots, \text{Head}_H) \mathbf{W}^o,
    \end{aligned}
\end{equation}
By performing the operation many times like this, many types of messages are passed between the nodes.
One can also think of this as constructing a multi-graph with different edge types each facilitated by a different head.
Both self and cross attention can be multi-headed where multi-headed self-attention (MHSA) is defined as $\text{MHSA}(\mathbf{X}) = \text{MHA}(\mathbf{X}, \mathbf{X})$.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/transformers/mha.pdf}
    \caption{Multi-headed attention can be thought of as constructing a multi-graph with different edge types.}
    \label{fig:mha}
\end{figure}

While it is not a mathematical requirement, in practice the MHA operation is typically non-resizing, the output matches the input tensor $\mathbf{X_r}$ in shape $(N_r, d_x)$.
The SPDA mechanism requires the query and key dimensions to be the same, but in most implementations the value dimension is also matches.
This means that a single MHA function is parametrized by four weight matrices each with shape $(d_x, d_x)$.
Thus the only hyperparameter for the block is the number of heads $H$, which is required to divide into the node dimension.
One can change value and query projections from linear to affine transformations by including bias terms to increase the expressivity of the model, but the bias of the key operation is completly redundant~\citetemp{RoleOfBiases}.

One of the other keys to the transformer's success is the ease of implementation.
Many of the GNN variants require custom libraries and are difficult to implement from scratch.
However MHA can be implemented in PyTorch using standard linear algebra operations as shown in \Cref{code:attention}.
All heads are computed in parallel using smart reshaping of the tensors.
This is quite significant, as it eases the process of designing, building, testing, and exporting models.
\begin{figure}
    \centering
    \scriptsize
    \begin{minted}{python}
    class Attention(nn.Module):
        def __init__(self, dim: int, num_heads: int):
            super().__init__()
            self.NH = num_heads
            self.HD = dim // num_heads
            self.scale = self.HD**-0.5
            self.wk = nn.Linear(dim, dim)
            self.wq = nn.Linear(dim, dim)
            self.wv = nn.Linear(dim, dim)
            self.wo = nn.Linear(dim, dim)

        def forward(self, xr: T.Tensor, xs: T.Tensor, bias: T.Tensor | int = 0) -> T.Tensor:
            q, k, v = self.wq(xs), self.wk(xr), self.wv(xs)              # project
            shape = (xr.shape[0], -1, self.NH, self.HD)                  # split heads
            q, k, v = (t.view(shape).transpose(1, 2) for t in (q, k, v))
            attn = (q @ k.transpose(-2, -1))                             # attention weight
            attn = (attn * self.scale + bias).softmax(dim=-1)
            x = attn @ v                                                 # aggregate
            x = x.transpose(1, 2).reshape(B, N, D)                       # mix heads
            return self.wo(x)
    \end{minted}
    \caption{A simple implementation of a multi-headed attention block in PyTorch.}
    \label{code:attention}
\end{figure}

\subsubsection{Efficiency of Scaled Dot-Product Attention}

SDPA is arguably the most efficient form of expressive message passing suitable for fully connected graphs.
To show this it is worth starting from the simplest GNN variant, the graph convolutional network (GCN)~\citetemp{GCN}, and seeing why it fails.
The GCN is a form of NLNN where each node sends the exact same message to all other nodes; a simple linear transformation of its own attributes.
The features are then replaced by a sum over incoming information.
A GCN layer can be written as (without normalization factors),
\begin{equation}
    \begin{aligned}
    \mathbf{e}_k' &= \mathbf{W} \mathbf{x}_{s_k}, \\
    \mathbf{x}_i' &= \sum_{k:r_k=i} \mathbf{e}_k',
    \end{aligned}
\end{equation}
for a single learnable weight matrix $\mathbf{W}$.
Lets consider the shapes of the tensors in this operation.
If we represent the set of nodes as a real valued tensor, it will have shape $(N, D)$, where $D$ is the dimensionality of the attributes.
The operation $\mathbf{W}: (N, D) \rightarrow (N, D)$ is efficient, but why this fails to form good representations is clear if we consider the case where the graph is fully connected and the number of edges is $N^2$.
Since all nodes have access to all others, including themselves, however after a single update step every node in the set will contain the same information, taking the over-smoothing problem to the extreme.

To prevent this from happening, each message must be unique for each possible pairing of nodes.
So the issue becomes; what is the most efficient way to parameterize a message such that the output of the message passing must be a tensor of shape $(N, N, D)$?
Here, the first dimension is over the sender nodes and the second dimension is the receiver nodes.
The pooling operation would act over the first dimension to produce a tensor of shape $(N, D)$.
\begin{align}
    \text{??}: (N, D) & \rightarrow (N, N, D) \\
    \text{pool}: (N, N, D) & \rightarrow (N, D)
\end{align}
The GN-Block offers a way to perform this by making the message depend on a concatenation of the sender and receiver attributes.
This concatenation must be done for all possible pairings before passing this tensor through an MLP.
The operations and dimensions of the tensors of such a block are,
\begin{align}
    \text{concatenate}: (N, D) & \rightarrow (N, N, 2D) \\
    \mathbf{MLP}: (N, N, 2D) & \rightarrow (N, N, D) \\
    \text{sum}: (N, N, D) & \rightarrow (N, D)
\end{align}
Even if we were to use a single linear layer rather than an MLP, having both the input and output tensor be $O(N^2)$ is computationally expensive.

One way to mitigate this is to factorize the message into two learnable components, the content and the weight.
Only one of these needs to be unique for each pairing and since the weight is a scalar and the content is a vector, it is much more efficient to allow the content to be degenerate.
This is exactly the approach of GATs~\citetemp{GAT}, which perform the update,
\begin{align}
    \mathbf{W_1}&: (N, D)  \rightarrow (N, D) \\
    \text{concatenate}&: (N, D)  \rightarrow (N, N, 2D) \\
    \mathbf{W_2}&: (N, N, 2D)  \rightarrow (N, N) \\
    \text{matrix multiply}&: (N, N), (N, D)  \rightarrow (N, D)
\end{align}
However, there is still the issue that this input and output of the learnable layers are $O(N^2)$.

The $(N, N)$ matrix of attention weights needs to be learnable from the original $(N, D)$ node attributes and must be allowed to be unique for each pairing.
Therefore, there must be a distinction between the sender and receiver nodes, this makes the required map $a: (N, D), (N, D) \rightarrow (N, N)$.
There are few operations that can achieve this, and none so efficient as a dot product.
Using linear projections for the initial embeddings means that the K, Q, V tensors can be computed in parallel.
\begin{align}
    \mathbf{W}: (N, D) & \rightarrow (N, 3D) \\
    \text{split}: (N, 3D) & \rightarrow (N, D), (N, D), (N, D) \\
    \text{scale-dot-softmax}: (N, D), (N, D) & \rightarrow (N, N) \\
    \text{matrix multiply}: (N, N), (N, D) & \rightarrow (N, D)
\end{align}
While the other parts of the update do require $O(N^2)$ operations, crucially the input and output of the learnable layers dependent is only $O(N)$.

The argument made here is that one will struggle to find a more simple and efficient way to parameterize the message passing step in a GNN that does not lead to over-smoothing when fully connected.
The philosophy of the transformer is similar to the philosophy of composing MLPs from linear layers, in that the composition of simple, highly parameterized functions, is what provides expressivity.
By and large the most complex component of the operation is the softmax function, but there exist algorithms to calculate this online without having to express the full attention matrix~\citetemp{FlashAttention}.
Contrary to popular claims, self-attention with these mechanisms is non $O(N^2)$ in memory complexity~\citetemp{MemoryComplexity}.
Some more recent models large models have even done away with the softmax, citing better performance beyond a certain model scale~\citetemp{Reformer, LinearAttetnion}.

\section{Transformer Variants}

There are many types of transformer variants, too much to cover in this thesis.
However, common similarities can be drawn between them.
Furthermore, many of the terminology used to describe the layers is ambiguous and overlaps.
A single transformer block has two main types of sub-blocks, the message passing layer performed with MHA, and node update layers performed using MLPs, often confusingly called FFNs.
These sub-blocks are combined, and repeated in various forms to create the larger transformer block.
The MHA layers are configured to be either self-attention or cross-attention as needed, and they are stacked with the FFN layers usually in series~\citetemp{AttentionIsAllYouNeed}, but sometimes in parallel~\citetemp{PALM}.
Another distinguishing feature is the use of residual connections and normalization layers to stabilize training.
But even these can be configured in many ways.
With such a large zoo of transformer variants, with empirical performance being the only guide, it often takes time for best practices to emerge.

The MLP in most transformer layers only has a single hidden layer, which expands the dimensionality of the node attributes by a multiplicative factor $M$.
The original implementation used $M=4$ and a ReLU activation function,
\begin{equation}
    \text{FFN}_\text{ReLU}(\mathbf{x}_i) = \max\left(0, \mathbf{W}_2 \text{ReLU}(\mathbf{W}_1 \mathbf{x}_i + \mathbf{b}_1) + \mathbf{b}_2\right),
\end{equation}
though these were quickly switched to either GELU of SiLU activations~\citetemp{GELU, SiLUTransformers}.
Modern transformers also use Gated Linear Units (GLU)~\citetemp{GLU}, a network layer involving the element-wise multiplication of two linear projections, one of which is passed through an activation function.
Using a Swish/SiLU activation function seems to be the most popular choice~\citetemp{SwiGLU}, the node update using a SwiGLU layer is written as,
\begin{equation}
    \text{FFN}_\text{SwiGLU}(\mathbf{x}_i) = \mathbf{W}_3(\text{Swish}(\mathbf{W}_1 \mathbf{x}_i + \mathbf{b}_1) \otimes (\mathbf{W}_2 \mathbf{x}_i + \mathbf{b}_2)) + \mathbf{b}_3.
\end{equation}
Many transformers also use the mixture-of-experts approach, where there exist many FFN layers, each with different parameters, and a gating mechanism to direct certain nodes to certain experts~\citetemp{MoE}.
It is interesting that while the attention mechinism is often hailed as the key to the transformer's sucess, the vast majority of the transformer parameters are in these node update layers.

To stabilize the training of deep models, the transformer encoder uses residual connections and normalization layers.
The residual connection is used to circumvent each sub-block, and in the original implementation used LayerNorm~\citetemp{LayerNorm} after the residual connection.
This was discovered to be over squashing the signal and most modern transformers now use PreNorm configuration where the normalization is done before each sub-block~\citetemp{PreNorm}.
Many models also use RMSNorm as a faster alternative~\citetemp{RMSNorm}.
Furthermore, extra normalization within the sub-blocks is common.
It is also common to initialize the weights of the final layers in each sub-block to zero, or include extra layers initialised close to zero, such that upon initialization the model behaves like an identity function.
This greatly helps to stabilize deeper models.

The transformer encoder (TE) is a standard GNN where the message passing step is performed using multi-headed attention.
It acts on a single set of nodes, and the output of the block is the updated node attributes.
A single TE-Block as shown in \Cref{fig:transformer} is a GN-Block with no edge or global features and the message passing step is done using MHSA.
The block update on the full tensor $\mathbf{X}$, using the PreNorm configuration is,
\begin{equation}
\begin{aligned}
    \mathbf{X} & \leftarrow \mathbf{X} + \text{MSHA}(L_\text{norm}(\mathbf{X})), \\
    \mathbf{X} & \leftarrow \mathbf{X} + \text{FFN}(L_\text{norm}(\mathbf{X})).
\end{aligned}
\end{equation}
A full TE is composed of many TE-Blocks stacked in series forming a permutation equivariant set to set transformation.
For many tasks, the TE is the main feature extractor.

If one wanted to condition the update of one set on another, they could include a multi-headed cross attention operation in the block.
This is the original implementation of the transformer decoder (TD)~\citetemp{AttentionIsAllYouNeed}.
The update is written as,
\begin{equation}
\begin{aligned}
    \mathbf{X}_r & \leftarrow \mathbf{X}_r + \text{MHSA}(L_\text{norm}(\mathbf{X}_r)), \\
    \mathbf{X}_r & \leftarrow \mathbf{X}_r + \text{MHA}(L_\text{norm}(\mathbf{X}_r), \mathbf{X}_s), \\
    \mathbf{X}_r & \leftarrow \mathbf{X}_r + \text{FFN}(L_\text{norm}(\mathbf{X_r})).
\end{aligned}
\end{equation}
The transformer encoder and decoder are often combined to form a set to set transformation that first extracts features from the input set, and the conditions the output set on these features.
This set-to-set transformation is the basis of many generative of sequence-to-sequence models.

\subsection{Edge Attributes, Biases, and Sequences}

When one views the transformer as a simple GNN, the inclusion of edge information become natural.
For message passing on a single graph, when not represented sparsely, the edge information is stored in an $\mathbf{E} \in \mathbb{R}^{N \times N \times d_e}$ tensor.
In the self attention operation in \cref{eq:attention}, the attention matrix defined by,
\begin{equation}
    \mathbf{A} = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{d_k} + \mathbf{B} \right),
\end{equation}
which has the shape $(N, N)$.
When extending this operation to multi-headed attention it gains an extra dimension $(N, N, H)$ matching the required shape of the edge tensor, in fact it is the only valid candidate within the entire operation.
The query and key tensors $\mathbf{Q}$ and $\mathbf{K}$ stem from projections of the input, but $\mathbf{B}$ must have shape which is at least broadcastable to $(N, N, H)$.
Thus edge information can be included in a transformer through the bias term in the attention operation.
This is used in various cases.

The most common use of the bias tensor is to set it manually to negative infinity for certain pairs of nodes and for all heads.
This forces the attention weight to be zero, after the softmax operation, killing the weight of the message entirely.
This enables the transformer to run on graphs which are not fully connected.
It is also one of the main mechanisms for adapting the transformer to work on sequences, where the nodes are presented in a specific order.
By making the bias matrix a lower triangular matrix with above diagonal entries set to negative infinity, each node can only receive messages from those preceding it.
This is also called a causal mask, and is used extensively generative models as it lends itself to autoregressive sampling.
One of the main reasons why the transformer is so successful compared to other sequence models such as an RNN is that a causal attention mask allows it to parallelize training for each element in the sequence with a single forward pass.
During generation of course the model must be run sequentially, but this is a small price to pay for the increase in training speed.
Many sequence generating models are called decoder only, however in the presented framework, these models are transformer encoders with a causal mask.

While can be shown that a causal mask is enough enable the transformer able to run on sequence data, it is often paired with positional encoding.
Absolute positional encoding (APE) adds to the node attributes knowledge of their position within the sequence.
A unique tensor $\mathbf{p}_i \in \mathbb{R}^{d_x}$ is constructed for each posible observed position $i$ and is added to the node attributes before the first transformer block, $\mathbf{x}_i \leftarrow \mathbf{x}_i + \mathbf{p}_i$.
The original transformer used fixed sinusoidal functions of different frequencies to encode the position, but more recent models allow $\mathbf{p}_i$ to be fully learnable~\citetemp{GPT}.
PE is also used for images in, often called spatial encoding, where the 2D coordinates of a patch are individually encoded and added to the patch representation.

APE is not effective for allowing the model to capture the relative position of the elements in the sequence, and for this we turn to relative positional encoding (RPE)~\citetemp{RPE}.
RPE is used to bias the attention matrix and can thus be thought of as a form of edge information.
As with all transformer features, there are many approaches to RPE.
One such method to parameterise $B$ as a Toeplitz matrix, where the elements of the matrix are learnable~\citetemp{RPE}.
ALiBi~\citetemp{ALiBi} fixes the elements of this matrix to be simply $B_{ij} = m\times(i - j)$, for a fixed per-head scalar $m$.
Rotary Positional Encoding is a form of RPE where the query and key tensors are rotated by a fixed amount before the attention operation~\citetemp{RotaryPE}.

Other than positional encoding, transformers can incorporate edge information that are not dependant on order.
The Particle Transformer (PartT)~\citetemp{PartT} uses a transformer encoder to embed a set of particles, defined by their kinematics.
For each pair of particles, specific edge features are constructed using Lund Plane variables~\citetemp{LundPlane}.
The model uses an MLP to embed these edge features into a tensor of shape $(N, N, H)$, which becomes the attention bias in each of the TE-Blocks.

\subsection{Global Attributes and Conditioning Graph Updates}

Global attributes are common features in graphs, and are often used to condition the main node update or are used as outputs of the model.
Global attributes in a GNN are often updated via pooling information across all nodes in the graph $ \mathbf{u} = \rho^{v \to u}(\mathbf{X})$.
This is done in the transformer by simply summing the node attributes.
However, keeping with the theme, the cross-attention mechanism provides a pooling operation that is more expressive, even when the cardinality of the receiving set is one.
This is often called class attention~\citetemp{GoingDeeper}, and in many vision transformer models in order to classify the image as a whole.
The class attention operation can be described as,
\begin{equation}
    \mathbf{U} \leftarrow \mathbf{U} + \text{MHA}(\mathbf{U}, \mathbf{X}),
\end{equation}
where the initial embeddings of the global attributes are learnable.
This process can also be repeated to perform multiple rounds of pooling.

An alternative method relies on the fact that since most transformers act on fully connected graphs, information is not localized.
Therefore, one can simply append a new global toke (often called a class token) to the set which is updated in the same way as the other nodes~\citetemp{VisionTransformers}.
Then to classify the set, class token is read off from the final layer of node attributes.
Recent work on vision transformers have shown that using multiple global tokens in this way, called registers~\citetemp{Registers}, can improve the performance of the model.
Registers can even be ignored from the model outputs, which means that they provide the model with a temporary memory that can be used to store global information.
This is because it was observed that the models had a tendency to overwrite patches with low local information content.

One can use global attributes or extra conditional information to augment the message passing step.
If the global attributes are a set, then cross attention can be used to pass information back and forth between the nodes and global attributes~\citetemp{PCDroid}.
If the global attribute is a single tensor, then cross attention will not suffice.
This is due to the softmax operation in the attention mechanism.
If there is only one sender node, then the attention weight will always be one.
Instead, the global attribute can be concatenated directly in different parts of the block.
This is commonly seen in diffusion models where TE-Blocks must incoroprate time information, expressed as an encoded vector.
This can be done by concatenating the $mathbf{u}$ vector to all node attributes during the MHA step~citetemp{DiffT} or the MLP step~citetemp{PCJedi}.
However the most effective way has been to combine adaptive normalization and residual gating mechanisms~citetemp{DiT}.
Here the global attribute is passed through three projections of the same dimensionality as the node attributes.
It is then used to scale, shift and gate the update,
\begin{equation}
    \begin{aligned}
    & \mathbf{u}_\text{scale}, \mathbf{u}_\text{shift}, \mathbf{u}_\text{gate} \leftarrow \text{MLP}(\mathbf{u}), \\
    & \mathbf{x} \leftarrow \mathbf{x} + \mathbf{u}_\text{gate} \otimes f \left(L_\text{norm}(\mathbf{x} \otimes \mathbf{u}_\text{scale} + \mathbf{u}_\text{shift})\right).
    \end{aligned}
\end{equation}
where $f$ is either the FF or MHA operation within the block.
One can also initialise the MLP in such a way that the $\mathbf{u}_\text{gate}$ is close to zero at the start of training and that the model is initially an identity function.

\chapter{Flavour Tagging with Graphs and Transformers}

In this chapter we describe the design and implementation new graph networks for flavour tagging.
These models build on GN1, a graph network which uses GATv2 system of message passing~\cite{GATv2}, and can be thought of as an architecture optimization of the already successful flavour tagger used by ATLAS.
The first new model is called GN++ and is based on the full GN-Block architecture described in \Cref{sec:gn_block}, complete with persistent edge information and global attributes.
The second new model is called Spice and is based on the transformer encoder architecture, described in \Cref{sec:transformer}, with the message passing step done using multi-headed SDPA.
Spice would prove to be so performant and efficient that it would form the basis of the new GN2 flavour tagger adopted by the ATLAS collaboration.
One of the observations made over the course of this project is that the rate of progress in machine learning is much faster than in high energy physics.
At the time Spice was being developed and presented to the flavour tagging group, GN1 was still undergoing collaboration.
At the time of writing, GN2 is now being calibrated and significant effort is being use to keep it up to date with the latest developments in machine learning.

\section{Datasets}

The same training and test samples used for the previous flavour taggers, DIPS and GN1, were used.
Further information can be found in Refs. \cite{GN1, AlexThesis}.

Two simulated datasets are used in this work to train and evaluate the models.
The first contains $\ttbar$ events and this covers the low $\pt$ phase space for training the taggers.
The simulation settings for the $\ttbar$ sample are derived from studies to best match jet multiplicity and top quark momentum distributions in data~\cite{ttbar1, ttbar2}.
The second sample contains $Z'$ events included to enhance the training statistics in the high $\pt$ phase space.
The $Z'$ boson is a non-SM particle where both the cross-section of the hard-scatter process and the branching fractions were specifically designed to produce a relatively uniform distribution of jets with $\pt$ up to $5~\TeV$ split equally between $b-$, $c-$, and light-jets.

\subsection{Monte Carlo Samples}

Both datasets are initiated by simulated proton-proton collisions at a center-of-mass energy of $\sqrt{s} = 13$ TeV.

Hard scatter events for the $\ttbar$ sample were generated at next-to-leading order using \textsc{PowhegBox v2} \cite{Powheg1, Powheg2, Powheg3} interfaced with \textsc{Pythia 8.230} \cite{Pythia8} using the \textsc{A14} tune \cite{A14} for parton showering and hadronization.
For the parton distribution functions, the \textsc{NNPDF3.0NNLO} \cite{PDF3.0} set was used for the matrix element calculation while the \textsc{NNPDF2.3LO} \cite{PDF2.3} set was used for the showering.
The cut-off scale parameter for the first-gluon-emission $h_{\text{damp}}$ was set to 1.5 times the top quark mass of $m_t = 172.5~\GeV$.
The $Z'$ sample was generated and showered using \textsc{Pythia 8.212} with the \textsc{A14} tune and the \textsc{NNPDF2.3LO} set of parton distribution functions.

For both datasets, decays of $b-$ and $c-$hadrons are done using the \textsc{EvtGen v1.6.0} package \cite{EvtGen}.
Detector response is performed using the \textsc{Geant4} simulation package \cite{Geant4} with the \textsc{ATLAS} simulation setup \cite{ATLASSim}.
Pileup is modelled by overlaying extra minimum bias events using the \textsc{Pythia 8.160} generator with the \textsc{A3} tune \cite{A3} and the \textsc{NNPDF2.3LO} parton distribution functions.
These signals are combined with the hard scatter events during the digitization step.
Interactions with heavy flavour hadrons and the detector material was not simulated but accounted for in correction factors and systematic uncertainties.
In-time pileup is modelled using an average of 40 interactions per bunch crossing, while out-of-time pileup is modelled using bunch crossings before and after the primary interaction.

\subsection{Selection and Sampling}

The ATLAS flavour tagging uses PFLow jets, which are described in \Cref{sec:jet_reconstruction}.
Jets are required to have $\pt > 20~\GeV$ and $|\eta| < 2.5$.
All jets with $\pt < 60~\GeV$ and $|\eta| < 2.4$ must also pass the tight working point of the JVT algorithm in order to minimise contributions from pileup.
Jets are also discarded is they are found to overlap with a generator-level lepton from a $W$ or $Z$ decay.

The training dataset is a mixture of 70\% $\ttbar$ and 30\% $Z'$ events.
To decorrelate the tagger with respect to the jet $\pt$ and $\eta$, jets are also resampled such that the truth matched $b-$, $c-$, and light-jets have identical distributions in these variables.
After resampling the training set contained a total of 120 million jets split equally between the three jet flavours.
During training, 5\% of this was reserved for hold out validation.
Two statistically independent test samples are each containing 1 million $\ttbar$ and $Z'$ jets respectively.
No resampling is done on the test datasets.

\section{Model Design}

All of the proposed models can be thought of as variations or extensions of GN1.
GN1 introduced the idea of a monolithic graph neural network which would simultaneously perform jet tagging, vertexing, and track identification.
All the proposed extensions follow this same setup but with updates to the message passing step as well as latest trends in architecture design.
From Ref.~\cite{RelationalInductiveBiases} and the design of the GN-Block the proposed updates include, persistent edge information, persistent global information, and node feed-forward layers.
From the modern network design and the current trend of transformers, we include multi-headed SDPA, PreNorm, and residual additive connections.

\section{Input Features}

The inputs to all the models presented in this section are the same.
They include two kinematic jet variables, $\pt$ and $\eta$, and up to 40 tracks associated with the jet.
Each track is represented by a vector containing 21 features, detailed in \Cref{tab:track_features}.
% Almost all \ttbar jets contain less than 40 tracks, but a significant fraction of $Z'$ jets contain more, as shown in \Cref{fig:track_multiplicity}.
For these samples only the 40 tracks with the highest IP significance $s(d_0)$ are retained.

\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \midrule
        \multicolumn{2}{c}{Jet Level Inputs Description} \\
        \midrule
        $\pt$ & Jet transverse momentum \\
        $\eta$ & Signed jet pseudorapidity \\
        \midrule
        \midrule
        \multicolumn{2}{c}{Track Level Inputs} \\
        \midrule
        $q/p$ & Track charge divided by momentum \\
        d$\eta$ & Pseudorapidity of the track, relative to the jet $\eta$ \\
        d$\phi$ & Azimuthal angle of the track, relative to the jet $\phi$ \\
        $d_0$ & Closest distance from the track to the PV in the longitudinal plane \\
        $z_0 \sin \theta$ & Closest distance from the track to the PV in the transverse plane \\
        $\sigma(q/p)$ & Uncertainty on q/p \\
        $\sigma(\theta)$ & Uncertainty on track polar angle $\theta$ \\
        $\sigma(\phi)$ & Uncertainty on track azimuthal angle $\phi$ \\
        $s(d_0)$ & Lifetime signed transverse IP significance \\
        $s(z_0)$ & Lifetime signed longitudinal IP significance \\
        nPixHits & Number of pixel hits \\
        nSCTHits & Number of SCT hits \\
        nIBLHits & Number of IBL hits \\
        nBLHits & Number of B-layer hits \\
        nIBLShared & Number of shared IBL hits \\
        nIBLSplit & Number of split IBL hits \\
        nPixShared & Number of shared pixel hits \\
        nPixSplit & Number of split pixel hits \\
        nSCTShared & Number of shared SCT hits \\
        nPixHoles & Number of pixel holes \\
        nSCTHoles & Number of SCT holes \\
        \bottomrule
    \end{tabular}
    \caption{Input features for the Graph Neural Network Flavour Tagger.}
    \label{tab:track_features}
\end{table}

% - Describe GN1
% - Describe the 3 tier loss
% - Describe the full GNN model
% - Describe the proposed transformer model

\section{Training}

% - Training highlights
% - Loss curves
% - All training parameters

\section{Performance}

% - Output distributions
% - ROC curves
% - Rejection values

\section{Applications}

% - GN2X
% - Current model architecture
% - Current training set
% - Current performance









