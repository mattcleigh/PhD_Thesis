\chapter{Probabilistic Generative Models}

It is worth highlighting that there are many overlapping definitions and conflicting terminology in the field of generative models~\cite{DiscriminativeVsGenerative, MachineLearningDiscriminative}, so we will attempt to directly define the terms used in this thesis.
We assume the existence of some unknown probability density function $p_\X(\x): \mathcal{X} \rightarrow \mathbb{R}$ over some random variable $\X \in \mathcal{X}$ which is used to populate a dataset $\mathcal{D} = \{\x_i\}_{i=1}^N$.
A probabilistic generative model (PGM) is a parametrized model fit using $\mathcal{D}$ that allows for the efficient generation of new samples following the same distribution.
There exist many types of PGMs, and some like Generative Adversarial Networks (GANs)~\cite{GenerativeAdversarialNetworks} only allow for the generation of samples, while others like Normalising Flows (NFs)~\cite{VariationalInferenceNormalizing} provide an explicit approximation of the density $p_{\theta}(\x) \approx p_\X(\x)$.

Quite often we are also interested in approximating a conditional distribution $p_{X}(\x|\con)$ where $\con$ is some context variable.
This is useful for many tasks in machine learning, such as text-to-image generation~\cite{Dalle}.
We can also describe supervised learning as conditional generation and the building PGMs where, in contrast to the notation in \Cref{ch:supervised}, $\x$ is now the target variable and $\con$ is the input data.

In this section we cover the theory behind PGMs used in later in this thesis, namely NFs and diffusion models.
These models are all nominally used to generate continuous data, which matches much of the data we are interested in generating in the context of HEP.
Generative models on discrete data, such as the autoregressive models used in text generation~\cite{GPT2} are not included.
An overview of the models is given in \Cref{fig:generative_models}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/transformers/pgms.pdf}
    \caption{An overview of the generative models covered in this thesis.}
    \label{fig:generative_models}
\end{figure}


\section{Older PGMs}

Over the course of this thesis the field of generative models has seen rapid development.
At the start of this thesis, two models dominated the field: Variational Autoencoders~\cite{AutoEncodingVariationalBayes} (VAEs) and GANs~\cite{GenerativeAdversarialNetworks}.
While these older models, especially GANs, produced very impressive results in select domains, the generation quality and fine control of the samples that we see today was not possible.
The current landscape of text-to-image~\cite{Imagen, Dalle, SD3}, and even text-to-video~\cite{ImagenVideo} generation would have been inconceivable just a few years ago.
While it is remarkable how quickly focus of these models has moved on, specifically to diffusion, it is still important to highlight them here, both for historical context and to understand the strengths of the models that have replaced them.

\subsection{Variational Autoencoders}

VAEs were introduced in 2013~\cite{AutoEncodingVariationalBayes} and quickly became a popular model for generative tasks, unsupervised learning and variational inference.
A VAE is an example of a latent variable model.
Here one introduces a latent random variable $\Z \in \mathcal{Z}$ which one marginalizes over to obtain the distribution of the data,
\begin{equation}
    p_\X(\x) = \int p_{\X,\Z}(\x, \z)\diff\z = \int p_\X(\x|\z)p_\Z(\z)\diff\z.
\end{equation}
The latent space usually covers all real values, $\mathcal{Z} = \mathbb{R}^d$, and the prior $p_\Z(\z)$ is taken to be a simple distribution, such as a diagonal Gaussian $\normal$.
Practically we would like to not enforce what information is stored in the latent variable.
The standard choice for modelling the likelihood is to use a parametrised Gaussian with constant variance,
\begin{equation}
    p_\X(\x|\z) \approx p_{\theta}(\x|\z) = \mathcal{N}(\x; G_{\theta}(\z), \sigma^2 I),
\end{equation}
where $G_{\theta}$ is a neural network called the decoder.

VAEs introduce an additional parametrized distribution to approximate the posterior which is also taken to be Gaussian with a diagonal covariance matrix,
\begin{equation}
    p_\Z(\z|\x) \approx q_\phi(\z|\x) = \mathcal{N}(\z; \boldsymbol{\mu}_\phi(\x), \boldsymbol{\Sigma}_\phi(\x)).
\end{equation}
Here the estimation of the mean and variance of the approximate posterior is done by a neural network called the encoder and gradients are backpropagated through the stochastic latent variable using the reparameterisation trick~\cite{AutoEncodingVariationalBayes}.

One can not train this model by simply maximizing the likelihood of the data, as the likelihood is intractable.
However, this form sets up a mapping between the data space to the latent space and back again and one can train the model by maximizing the evidence lower bound (ELBO).
Practically this involves minimizing two terms, the reconstruction loss and the KL-divergence between the approximate posterior and the prior,
\begin{equation}
        \mathcal{L}(\theta, \phi) = \mathbb{E}_{\z \sim q_{\phi}(\z|\x)}[||\x - G_{\theta}(\z)||^2] - \beta D_{KL}(q_{\phi}(\z|\x) || p_\Z(\z)),
\end{equation}
where $\beta$ is a hyperparameter that balances the two terms~\cite{BetaVAE}.

Typically the dimensionality of the latent space is much lower than the data space, so the information bottleneck means the model is forced to learn a compressed representation.
VAEs are simple to train but have a number of limitations.
The bottleneck means that the samples are often of low quality and lacking in high frequency detail.
The model is also sensitive to the tuning of $\beta$, often leading to a collapse into one of two regimes.
Either the prior loss dominates, all samples are encoded to the unit Gaussian and all information is lost.
Alternatively the reconstruction loss dominates, the posterior is non-Gaussian and extra steps are required to sample from it, such as fitting a kernel density estimator to the empirical distribution of the latent variables~\cite{KDEVAE}.

Conditional information can be injected into the VAE simply by combining the context variable with both the inputs to the encoder and decoder.
The compression of the latent means that it only stores orthogonal information to the context variable, which is useful for disentanglement~\cite{cVAE}.

VAEs were never particularly good at generating high quality samples, but they attempts were still made in HEP to use them for fast simulation of detector responses~\cite{VariationalAutoencodersJet, GraphVariationalAutoencoder, ParticlebasedFastJet}.
One other notable usecase for anomaly detection~\cite{VariationalAutoencodersAnomalous, DeepSetVAE, MassiveIssueAnomaly}.
Both of these applications produced mixed results and the field at large has moved on to more powerful models.


\subsection{Generative Adversarial Networks}

Up until recently, GANs were the state-of-the-art in generative models for producing high quality samples.
This was most notably demonstrated in the generation of high resolution images~\cite{StyleGAN2,StyleGAN3}.
GANs are based on a two-player min-max game between a generator $G_\theta(\z)$ and a discriminator $D_\phi(\x)$, each neural networks with parameters $\theta$ and $\phi$ respectively.
These networks contest with each other in a zero-sum game where the one network's loss is the other's gain.

The generator takes a random latent sample $\z \sim p_\Z(\z)$ and returns a synthetic sample $\hat{\x} = G_\theta(\z) \in \mathcal{X}$.
The discriminator takes samples from both the training set and the synthetic samples and returns a probability that the sample is from the training set, $D_\phi(\x) \in [0, 1]$.
The objective of the generator is to fool the discriminator into thinking the generated samples are real.
Given powerful enough networks, the unique Nash equilibrium of this game is a generator that produces the true data distribution and a uniform discriminator output for all real samples.

There are many variations for defining the specifics of this game and the loss functions, but arguably the most widely used is the original non-saturating loss~\cite{GenerativeAdversarialNetworks}.
This loss requires two passes through each network, one to update the discriminator and one to update the generator.
\begin{align}
    \mathcal{L}_{\text{D}}(\phi) &= -\mathbb{E}_{\x \sim \mathcal{D}}[\log D_{\phi}(\x)] - \mathbb{E}_{\z \sim p_\Z(\z)}\log(1 - D_{\phi}(G_{\theta}(\z))], \\
    \mathcal{L}_{\text{G}}(\theta) &= -\mathbb{E}_{\z \sim p_\Z(\z)}[\log D_{\phi}(G_{\theta}(\z))].
\end{align}
Other variants exist which use different loss derivations, such as the Wasserstein GAN~\cite{WGAN1} and the Geometric GAN~\cite{GeometricGAN}.
There is plenty of literature on which variants are best suited for different tasks and which actually converge to the desired Nash equilibrium~\cite{WhichTrainingMethods}.

The main difficulty of GANs is that they are difficult to train.
Often there is mode collapse, where the generator exploits some weakness in the discriminator and reproduces the same samples over and over.
Many tricks are required to stabalize training~\cite{WhichTrainingMethods}, such as minibatch discrimination~\cite{ProGAN} and gradient penalties~\cite{WGAN}.
Another drawback to GANs is that it is difficult to product a conditional generator $G_\theta(\x|\con)$.
Simply concatenating the context variable to both the generator and discriminator inputs leads to mixed results and the generator is prone to ignoring the context variable~\cite{cGAN}.

In HEP, GANs have mainly been trialed as a method for fast simulation~\cite{MPGAN, GAPT, CaloGAN, EPICGAN}.
However, the main issue is that GANs tend to not cover the full data distribution, and that most of the generation we are looking for in fast simulation is conditional;
detector response depends on the properties of the incoming particle, making GANs not optimal for this task.

\section{Normalizing Flows}

NFs are a popular tool for both variational inference~\cite{VariationalInferenceNormalizing, NormalizingFlowsProbabilistic}
and density estimation~\cite{NICENonlinearIndependent}.
Like GANs and VAEs, they prescribe a method to train a transformation from a simple latent distribution into one that matches the data.
However, unlike GANs and VAEs, normalizing flows allow for explicit likelihood evaluation, making them well suited for density estimation.

Like all latent variable models, NFs turn a simple latent distribution into a complex data distribution.
However, NFs strive to do this exactly via the change of variables formula.
Given two random variables of equal dimensionality $\X$ and $\Z$ related by a bijective function $f: \mathcal{X} \rightarrow \mathcal{Z}$, the probability densities $p_\X(\x)$ and $p_\Z(\z)$ are related by
\begin{equation}
    \label{eq:change_of_variables}
    p_\X(\x) = p_\Z(f(\x)) \left|\det D f(\x) \right|.
\end{equation}
Here $f(\x)$ is a differentiable bijective function with a differentiable inverse, otherwise known as a diffeomorphism, and $D f(\x)$ is the Jacobian of the transformation.
The density $p_\X(\x)$ is called a \textit{pushforward} of the density $p_\Z(\z)$ by the function $f$, $p_\X = f_\# p_\Z$.

Unlike GANs, a NF constructs an explicit approximation of data density by defining it as a pushforward of a simple latent distribution, typically Gaussian, through a parametrized neural network,
\begin{equation}
    p_{\X}(\x) \approx p_{\theta}(\x) = f_{\theta \#} p_\Z(\z) = p_\Z(f_{\theta}(\x)) \left|\det D f_{\theta}(\x) \right|.
\end{equation}
Like all neural networks, NFs are built from the composition of several simple layers, but unlike other neural networks, each \textit{flow layer} has to be a diffeomorphism.
This is because the composition of two diffeomorphisms is itself a diffeomorphism, furthermore the Jacobian of the full transformation is simply the product of the Jacobians of the individual layers,
\begin{equation}
    f = f_1 \circ f_2 \circ \ldots \circ f_L \quad \Rightarrow \quad \det D f(\x) = \prod_{i=1}^L \det D f_i(\x_i).
\end{equation}
This composition property allows us to construct arbitrarily complex transformations, allowing for the approximation of any data distribution no matter how complex given a simple latent distribution~\cite{bogachev2005triangular}.

NFs are typically trained via a maximum log-likelihood approach, or more specifically, they use a negative log-likelihood loss function,
\begin{equation}
    \label{eq:nf_loss}
    \begin{aligned}
        \mathcal{L}(\theta)
        \frach &= \mathbb{E}_{\x \sim \mathcal{D}}[-\log p_{\theta}(\x)] \\
        \frach &= \mathbb{E}_{\x \sim \mathcal{D}}\left[-\log p_{\Z}(f_{\theta}(\x)) - \log \left|\det D f_{\theta}(\x) \right|\right] \\
        \frach &= \mathbb{E}_{\x \sim \mathcal{D}}\left[-\log p_{\Z}(f_{\theta}(\x)) - \sum_{i=1}^L \log \left|\det D f_i(\x_i) \right|\right].
    \end{aligned}
\end{equation}
Note that due to the composition property of the Jacobian, the log determinant term is a sum of the log determinants of the individual layers, leading to essentially independent contributions per layer in the final loss.

During training, the flow is said to run in forward-mode, transforming samples from the data space to the latent space.
Once the model is trained, it can again be used in forward-mode to yield the density at any measured point, or it could be run in reverse mode $f^{-1}$, transforming samples from the latent space to the data space.

Conditional density estimation and sampling is a natural extension of NFs.
The only modification is that the context variable is included to the input of each layer in the flow.
This allows the model to learn the conditional distribution $p_{\X}(\x|\con)$ yields surprisingly good results and highly calibrated uncertainties that match the aleatoric uncertainty in the data almost exactly~\cite{SolvingInverseProblems, InferenceAstrophysicalParameters, ComposingNormalizingFlows, NormalizingFlowsProbabilistic}.
Standard regression, which operates under the assumption of a Gaussian likelihood, is often overly restrictive.
A more effective approach for almost all regression tasks involves framing them as conditional density estimation, where NFs emerge as one of the most suitable tools, particularly when the dimensionality is low.

While NFs allow for the exact calculation of the likelihood, they are not without their drawbacks.
The main issue is that NFs are computationally expensive and do not scale well to high-dimensional data.
Even with the specialized flow layer explained in the next section, the complexity of calculating the determinant of the transformation is prohibitive for data with more than a few hundred dimensions such as images.
Another notable feature of NFs is that they are designed to work with continuous data.
As NFs describe continuous morphs of data, if the latent distribution is connected, so too will any learned density.
This causes issues when attempting to model data with disconnected support.
Discrete data needs to be \textit{dequantized} before being fed into the flow otherwise singularities will occur during training.
Dequantisation essentially amounts to adding noise to the data.
This is notable in images where pixel values tend to be quantized to 8-bit integers~\cite{FlowImprovingFlowBased}.

In HEP, NFs have been widely adopted for many tasks most notably fast simulation, template building, and unfolding, which are covered in detail in \Cref{ch:fastsim}, \Cref{ch:templates}, and \Cref{ch:unfolding} respectively.

\subsection{Flow Layers}

Building a NF requires stacking together multiple flow layers.
As mentioned, each of these layers must be a diffeomorphism.
Typically, they would need to operate on real valued tensors $\x \in \mathbb{R}^d$ and optionally $\con \in \mathbb{R}^c$.
In addition, for practical applications each layer needs to be efficient to process, both in the forward- and reverse-mode, and the determinant of the Jacobian should be easy to compute.
The design of these layers is the core research problem in normalizing flows.

At the time of writing, the most popular flow layers are linear flows, coupling flows, autoregressive flows, residual flows, and continuous-time flows.
We describe the ones most relevant to this thesis.

\subsubsection{Linear Flows}

A simple, non-resizing linear transform is arguably the most basic flow layer $f(\x) = \W\x + \bias$, as the inverse is simply $f^{-1}(\z) = \W^{-1}(\z - \bias)$.
Only a small amount of reparameterisation is required to ensure that the matrix $\W$ is non-singular and thus invertible.
Linear flows are limited in their expressiveness, furthermore, the complexity of calculating $\det D f(\x) = |\det \W|$ is $\bigO(d^3)$ making them computationally expensive for high-dimensional data.

One can restrict the form of the matrix to make it easier to use, as shown in \Cref{tab:linear_flows}.
The most popular choices are the LU-Factorisation and for images a $1\times1$-convolution~\cite{GlowGenerativeFlow}.

\begin{table}[h!]
    \centering
    \caption{Complexity of using different forms of linear flows.}
    \label{tab:linear_flows}
    \begin{tabular}{ccc}
    \toprule
    \textbf{Type} & \textbf{Inverse} & \textbf{Determinant} \\
    \midrule
    Full & $\bigO(d^3)$ & $\bigO(d^3)$ \\
    Diagonal & $\bigO(d)$ & $\bigO(d)$ \\
    Triangular & $\bigO(d^2)$ & $\bigO(d)$ \\
    Block (c) Diagonal & $\bigO(c^3d)$ & $\bigO(c^3d)$ \\
    LU Factorized & $\bigO(d^2)$ & $\bigO(d)$ \\
    1x1 Convolution & $\bigO(c^3 + c^2d)$ & $\bigO(c^3)$ \\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Coupling Flows}

Coupling flows~\cite{NICENonlinearIndependent} describe a general approach to constructing highly expressive non-linear flows.
In a coupling flow, the input $\x$ is partitioned into two disjoint components $(\x_A, \x_B)$.
$\x_A$ is used to compute the parameters of a bijective transformation $h(\dot; \phi)$ which is applied to only $\x_B$.
This can be expressed as,
\begin{equation}
    \label{eq:coupling_flow}
    f(\x) = (\x_A, h(\x_B; \phi(\x_A))).
\end{equation}
As shown in \Cref{fig:coupling_flow}, this layer is entirely reversible so long as the coupling function $h$ is invertible, whereas the conditioner $\phi$ does not have this restriction.
Additionally, the Jacobian of the transformation is a block matrix given by,
\begin{equation}
    D f(\x) = \begin{pmatrix}
        \mathbb{I} & 0 \\
        \frac{\partial h(\x_B; \phi(\x_A))}{\partial \x_A} & \frac{\partial h(\x_B; \phi(\x_A))}{\partial \x_B}
    \end{pmatrix}.
\end{equation}
When calculating the determinant the entire bottom left block can be ignored making it simply the determinant of the invertible transformation $h$.
Thus, $\phi$ can be arbitrarily complex, without impacting the convertibility or the computational complexity of calculating the likelihood contributions of the layer.
It is common therefore to use a deep neural network for $\phi$, to maximise the expressivity of the layer.
Conditional information can easily be injected into the flow by incorporating the context variable to the input of the conditioner $\phi(\x_A, \con)$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/transformers/coupling.pdf}
    \caption{A diagram of a coupling flow.}
    \label{fig:coupling_flow}
\end{figure}

Note that the layer only transforms $\x_B$ and leaves $\x_A$ unchanged.
Thus, multiple coupling layers are typically stacked together, interleaved with a fixed random permutation layer or a linear flow to mix the elements of $\x$.
Equivalently one could also alternate the mask used to partition $\x$ between coupling layers.
It is standard invertible to set $h$ to be an elementwise transformation for ensure efficiency as most of the expressivity in the layer comes from $\phi$.
Early original coupling flows used an affine transformation for $h$~\cite{RealNVP}, but this required many layers to fit to complex data distributions.
The expressivity of the model can be greatly enhanced using more complex transformations, yet still elementwise bijections, such as rational quadratic splines~\cite{NeuralSplineFlows}.
Splines are especially useful for modelling multimodal data distributions, as single segments within the monotonic splines can become arbitrarily steep, allowing for regions of very low density.
This is demonstrated by \Cref{fig:samples}, which shows the output at of a flow with 4 coupling layers with spline transformations.
The learned densities are shown in \cref{fig:density}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/transformers/samples.pdf}
    \caption{The transformation of gaussian samples into a double-moon distribution (top) and a four-box distribution (bottom) using a flow with 4 coupling layers. Each coupling layer can only transform one dimension of the data.}
    \label{fig:samples}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/transformers/densities.pdf}
    \caption{The learned (left) and real (right)densities of the double-moon distribution (top) and the four-box distribution (bottom) using a NF.}
    \label{fig:density}
\end{figure}

\subsection{Continuous Normalizing Flows}

% In a CNF, the transformation is defined by a differential equation parametrised by a time $t \in [0, 1]$
% Fixed end points are x(0) = 0 and x(1) = z.
% This gives the forward and inverse transformations as two time ordered ODEs,
% Put equation
% Once the transformation is defined, any numerical ODE solver can be used to transform samples from the data space to the latent space and back again.

Whereas in a standard NF the transformation composed by a fixed number of sub-transformations or layers, a continuous normalizing flow (CNF)~\cite{NeuralODE} defines a continuous process where each infinitesimal transformation depends on the current state of the data and the time index.
This leads to an ordinary differential equation (ODE) of variable $\x_t \in \mathbb{R}^d$ indexed by a time $t \in [0, 1]$ defined by a smooth time-varying vector field $u(\x_t, t)$,
\begin{equation}
    \label{eq:cnf}
    \diff \x_t = u_t(\x_t) \diff t.
\end{equation}
Note that we will use $u_t(\x_t)$ and $u(\x_t, t)$ interchangeably.
Given $\x_0 \sim p_0$, the vector field creates a pushforward $p_t = [\phi_t]_\# p_0$, where $\phi_t$ is the solution of the ODE,
\begin{equation}
    \label{eq:cnf_pushforward}
    \x_t = \phi_t(\x_0) = \x_0 + \int_0^t u_s(\x_s) \diff s.
\end{equation}
The time dependant density $p_t$ must satisfy the continuity equation,
\begin{equation}
    \label{eq:cnf_continuity}
    \frac{\partial p_t}{\partial t} = -\nabla \cdot (u_t p_t).
\end{equation}
The velocity field $u_t$ is said to be the \textit{probability flow} ODE~\cite{ScoreBasedGenerativeModeling} and $p_t$ is the \textit{probability path} generated by $u_t$.
The induced log-likelihood of the pushforward is given by,
\begin{equation}
    \label{eq:cnf_loglikelihood}
        \log p_t(\x_t)
    = \log p_0(\x_0) - \int_0^t Tr\left(D u_s(\x_s)\right)\diff s
    = \log p_0(\x_0) - \int_0^t (\nabla \cdot u_s)(\x_s) \diff s,
\end{equation}
which was proven in \textcite{NeuralODE}.

To make this model generative, one can set the initial condition to match the latent distribution $p_0 = p_\Z$, the desired end point at $t = 1$ to match the data distribution $p_1 = p_\X$, and parameterize the velocity field with a neural network $u_\theta(\x, t): \mathbb{R}^d \times \mathbb{R}_{+} \rightarrow \mathbb{R}^d$.
One can set .
Like any flow, the CNF can then be trained by maximizing the log-likelihood of the data,
\begin{equation}
    \label{eq:cnf_loss}
    \mathcal{L}(\theta) = \mathbb{E}_{\x \sim \mathcal{D}}\left[\log p_0(\x) - \int_0^1 (\nabla \cdot u_\theta(\x_t, t)) \diff t\right].
\end{equation}
Once the transformation is defined, any numerical ODE solver can be used to transform samples from the data space to the latent space and back again.
This introduces an extra level of complexity in evaluating the model.
A single trained model can have a wide range of behaviour depending on the choice of ODE solver, the number of integration steps, the use of adaptive step sizes.
With fewer steps, truncation error during generation can lead to a loss of sample fidelity.
With more steps, the computational cost of generation increases.

CNFs are highly expressive models, but the main issue is that they are slow to train.
The loss requires calculating the divergence of the vector field, which is slow to compute in many libraries\footnote{Libraries with advanced automatic differentiation, like JAX\cite{JAX}, can alleviate this issue to a degree}.
Early CNFs became trainable for low dimensions using the Hutchinson trace estimator~\cite{FFJORD}, which approximates the trace of the Jacobian with a single Monte Carlo sample.
Even with these optimisations, a single traning iteration still requires a full numerical integration of the ODE, and subsequent backpropagation through each step.
This makes CNFs impractical for high-dimensional data.

\section{Diffusion Models}

Over the past four years, diffusion models have emerged as the standard for generative high-dimensional data.
Their impact on the landscape of generative models has been profound, elevating the field from method papers on two-dimensional toy datasets to commercially viable high-resolution text-to-image generative models that have become ubiquitous on the internet.
The primary reason for this significant advancement is that diffusion models are exceptionally straightforward to train, especially when compared to CNFs and GANs.
Their training process is largely stable, with fixed supervised targets.
Furthermore, diffusion models allow for seamless incorporation of contextual generation, enabling fine control over the generated samples.
Finally, their superior sample generation capabilities results in high-resolution images that are nearly indistinguishable from real images, significantly outperforming all other PGMs in this regard.

Diffusion models emerged in 2015~\cite{DeepUnsupervisedLearning} where the authors were inspired by non-equilibrium statistical physics.
There are three main components to this type of model.
The first is a \textit{forward-process} that iteratively and systematically destroys structured data by adding noise.
Typically, this is non-learnable, simple to compute, and by the end of the process, the data is completely destroyed and indistinguishable from noise.
A model is then trained to learn the \textit{reverse-process}, which restores the structure of the data.
This too is iterative, and must be repeatedly applied to transform sample of pure noise, typically $\normal$, into a sample that matches the data distribution.
This defines the final component, the \textit{sampling procedure}, a single trained model can be sampled in many ways, with different trade-offs between speed and quality.
It took 5 years for this idea to be revisited, improved, and applied successfully to images in the form of the Diffusion Probabilistic Models (DDPM)~\cite{DDPM}.
The true research frenzy began in 2021 when diffusion models officially dethroned GANs as the state-of-the-art for conditional image generation~\cite{DiffusionBeatsGANS}.

There are many competing, overlapping, and sometimes equivalent frameworks used to define diffusion models.
They are described through the lense of Markov chains trained via variational inference~\cite{DDPM, DDIM}, score-based models with Langevin Dynamics~\cite{ScoreBasedGenerativeModeling, ElucidatingDesignSpace}, and flow matching with CNFs~\cite{FlowStraightFast, FlowMatchingGenerative}.
All of these models share similar training objectives, but differ in the mathematical derivations.
There have also been many attempts to unify these models under a framework~\cite{CM2, ElucidatingDesignSpace,UnderstandingDiffusionModels, StochasticInterpolants, FlowStraightFast}.
Since no single framework has emerged as the standard we will not attempt to unify them here, but we do use the term ``diffusion model'' as an umbrella term to describe all of these models in this chapter.
We chose to focus on two frameworks that are used directly in the projects of this thesis, namely score-based models~\cite{ScoreBasedGenerativeModeling} and flow matching models~\cite{FlowMatchingGenerative, FlowStraightFast}.

\subsection{Score-Based Generative Models}
\newcommand{\score}{\nabla_{\x_t} \log p(\x_t)}
\newcommand{\cscore}{\nabla_{\x_t} \log p(\x_t|\x_0)}
\newcommand{\unitime}{\mathcal{U}(0, T)}
\newcommand{\timeone}{\mathcal{U}(0, 1)}

Score-based generative modelling~\cite{ScoreMatching, ScoreBasedGenerativeModeling} is a framework that envelops many diffusion models such as DDPM~\cite{DDPM}.
The forward-process involves progressively applying Gaussian perturbations to a data sample $\x_0 \sim p_0(\x) = p_\X(\x)$\footnote{Note that the time index in diffusion models is typically inverted compared to CNFs, where $p_0$ is the noise/latent distribution.}.
Score-based models describe this process in the continuous limit with time index $t \in [0, T]$ such that it can be modelled as an SDE,
\begin{equation}
    \label{eq:sde}
    \diff \x_t = \underbrace{f(\x_t, t)}_{\mathclap{\text{drift}}} \diff t + \underbrace{g(t)}_{\mathclap{\text{diffusion}}} \diff \w_t,
\end{equation}
where $\w_t$ is the standard Wiener process (Brownian motion) and $f: \mathbb{R}^d \times \mathbb{R}_{+} \rightarrow \mathbb{R}^d$ and $g: \mathbb{R}_{+} \rightarrow \mathbb{R}$ are the drift and diffusion functions respectively.
Note that if $|g(t)| > 0$ the data signal is progressively corrupted.

While the forward-process is non-learnable, it is not quick to solve without numerical integration.
However, if the drift function is affine with respect to $\x_t$, the transition kernel becomes Gaussian,
\begin{equation}
    \label{eq:gaussian_kernel}
    f(\x_t, t) = f(t)\x_t \quad \Rightarrow \quad p_{0t}(\x_t|\x_0) = \mathcal{N}(\x_t; s(t)\x_0, \sigma(t)^2 \mathbb{I}),
\end{equation}
where $s(t)$ and $\sigma(t)$ are time-dependent scaling factors, referred to as the signal rate and noise rate respectively, and can be derived for specific forms of $f(t)$ and $g(t)$~\cite{sarkka2019applied}.
Alternatively one can select specific forms for the scaling factors and derive the drift and diffusion functions~\cite{ElucidatingDesignSpace}.
Either way, this choice uniquely defines the \textit{diffusion schedule} for the process, and for generative models one typically constructs it such that,
\begin{equation}
    \label{eq:diffusion_schedule}
    \lim_{t \rightarrow 0} s(t) = 1, \quad \lim_{t \rightarrow 0} \sigma(t) = 0, \quad \lim_{t \rightarrow T} \frac{s(t)}{\sigma(t)} = 0,
\end{equation}
as this ensures that $p_0 = p_\X$ and that $p_\Z \approx p_T = \mathcal{N}(0;, \sigma(T) \mathbb{I})$.

Using the transition kernel defined in \Cref{eq:gaussian_kernel}, one is able to sample from the forward-process at any arbitrary time $t$ using the reparametrisation trick,
\begin{equation}
    \label{eq:reparametrisation}
    \x_t = s(t)\x_0 + \sigma(t) \e \quad \text{where} \quad \e \sim \normal \quad \text{and} \quad \x_0 \sim p_0(\x).
\end{equation}

Remarkably, the SDE in \Cref{eq:sde} has an exact inverse~\cite{ReversetimeDiffusionEquation} which can be used to define the reverse-process,
\begin{equation}
    \label{eq:reverse_sde}
    \diff \x_t = \left[ f(\x_t, t) - g(t)^2 \score \right] \diff t + g(t) \diff \bar\w_t,
\end{equation}
where $\bar\w_t$ is a Wiener process with opposing sign and time flows from $T$ to $0$.
The only unknown term in \Cref{eq:reverse_sde}, $\score$, is referred to as the score function and thus approximating it is the main objective of a score-based generative model.
This is also known as score matching (SM)~\cite{ScoreMatching}.

One naive approach to perform SM is to use direct regression, resulting in the loss function,
\begin{equation}
    \label{eq:naive_loss}
    \mathcal{L}_{SM}(\theta) =
    \mathbb{E}_{t \sim \unitime}
    \mathbb{E}_{\x_t \sim p_t(\x_t)}
    \left|\left|S_\theta(\x_t, t) - \score\right|\right|^2,
\end{equation}
where $S_\theta$ is a neural network with trainable parameters $\theta$.
However, this loss is not practical as it requires the calculation of the very term we are trying to approximate.
One can instead use the denoising score matching (DSM) objective~\cite{ScoreMatching, SlicedScoreMatching} where the conditional score function is approximated,
\begin{equation}
    \label{eq:denoising_loss}
    \mathcal{L}_{DSM}(\theta) =
    \mathbb{E}_{t \sim \unitime}
    \mathbb{E}_{\x_0 \sim \mathcal{D}}
    \mathbb{E}_{\x_t \sim p_{0t}(\x_t|\x_0)}
    \left|\left|F_\theta(\x_t, t) - \cscore\right|\right|^2.
\end{equation}
Firstly, this replacement is valid because after taking all expectations, the gradients of the two loss functions are equivalent with respect to the model parameters~\cite{ScoreBasedGenerativeModeling},
\begin{equation}
    \nabla_{\theta} \mathcal{L}_{SM}(\theta) = \nabla_{\theta} \mathcal{L}_{DSM}(\theta).
\end{equation}
Secondly, this replacement is useful because the conditional score function does have a tractable form due to the transition kernel in \Cref{eq:gaussian_kernel}.
This is shown by,
\begin{equation}
    \begin{aligned}
        \label{eq:conditional_score}
        \cscore
        &= -\nabla_{\x_t} \log \left( \mathcal{N}(\x_t; s(t)\x_0, \sigma(t)^2 \mathbb{I}) \right) \\
        &= -\nabla_{\x_t} \frac{\left(\x_t - s(t)\x_0\right)^2}{2\sigma(t)^2} \\
        &= \frac{s(t)\x_0 - \x_t}{\sigma(t)^2} \\
        &= \frac{s(t)\x_0 - s(t)\x_0 - \sigma(t)\e}{\sigma(t)^2} \quad \leftarrow \quad \text{reparametrised sampling} \\
        &= - \frac{\e}{\sigma(t)}.
    \end{aligned}
\end{equation}
It is therefore also convenient to precondition the network output as,
\begin{equation}
    \label{eq:precondition}
    S_\theta(\x_t, t) = - \frac{\epsilon_\theta(\x_t, t)}{\sigma(t)}.
\end{equation}
This yields final loss function,
\begin{equation}
    \label{eq:noise_loss}
    \mathcal{L}_{DSM}(\theta) =
    \mathbb{E}_{t \sim \unitime}
    \mathbb{E}_{\x_0 \sim \mathcal{D}}
    \mathbb{E}_{\e \sim \normal}
    \frac{\gamma(t)}{\sigma(t)^2}
    \left|\left|\e_\theta(\x_t, t) - \e\right|\right|^2,
\end{equation}
where $\gamma(t) > 0$ is introduced as a reweighting factor to balance the loss across the diffusion schedule.
Thus, to train a score-based generative model, one only needs to train a network to approximate the noise used to corrupt data.
Note that by setting $\gamma(t) = \sigma(t)^2$ the loss is uniformly weighted and this often corresponds to good perceptual quality of the generated samples~\cite{VariationalPerspectiveDiffusionBased, VariationalDiffusionModels, ImprovedDenoisingDiffusion}.
Alternatively, one can use specific weighting schemes to allow training on the maximum likelihood objective~\cite{UnderstandingDiffusionModels}.

Once the model is trained, one can define the sampling procedure which starts from samples drawn from $p_Z$ and numerically integrates the reverse SDE in \Cref{eq:reverse_sde} from $t: T \rightarrow 0$ to optain samples matching $p_0$.
The integration can be done using the Euler-Maruyama method or stochastic Runge-Kutta methods~\cite{NumericalSolutionStochastic}.
This process takes time and the generation quality is highly sensitive to the number of integration steps as each step requires another forward pass through the neural network.
The large computational cost of generating a sample with a diffusion model is the main drawback of the approach when comparing it to single pass methods like NFs and GANs.

As a method to speed up generation, \textcite{ScoreBasedGenerativeModeling} proved the existence the probability flow ODE whose solutions share the same marginal densities $p_t(\x_t)$ as the SDE.
For the SDE defined in \Cref{eq:sde} it is given by,
\begin{equation}
    \label{eq:probability_flow}
    \diff \x_t = \left[f(\x_t, t) - \frac{1}{2} g(t)^2 \score\right] \diff t.
\end{equation}
Empirically, the probability flow ODE provides a larger tolerance to truncation errors, allowing for fewer integration steps and thus faster generation.
In addition, it enables the use of more advanced ODE solvers, provides deterministic encoding, and log likelihood computation via~\Cref{eq:cnf_loglikelihood}, indicating the strong connection between score-based models and CNFs.
Furthermore, while the probability flow ODE was originally introduced as an alternative method for generation, \textcite{ElucidatingDesignSpace} showed that the entire training objective of a score-based model can be derived purely from an ODE perspective.

\subsection{Conditional Flow Matching}

Conditional Flow Matching (CFM)~\cite{BuildingNormalizingFlows, FlowMatchingGenerative, FlowStraightFast} is an alternative framework which arrives at an equivalent training objective to score-based models and can be seen as a method to train CNFs without the need for numerical integration.
The derivation shares many parallels with that of score-based models and there is abundant literature that argues their equivalence~\cite{CM2, StochasticInterpolants}.
They however approach the problem without the use of SDEs, and like \textcite{ElucidatingDesignSpace} directly target the probability flow ODE, arguably yielding a simpler derivation.

Like CNFs, the goal of CFM is to learn the velocity field $u_t(\x_t)$ that induces the time dependent density $p_t(\x_t)$ adhering to the boundary conditions $p_1 = p_\X$ and $p_0 = p_\Z$\footnote{Note again that the time index is inverted compared to score-based models.}.
One can express this density as a marginal over a conditional probability path which propagates from a single data sample,
\begin{equation}
    \label{eq:conditional_path}
    p_t(\x_t) = \int p_1(\x_1) p_{t1}(\x_t|\x_1) \diff \x_1.
\end{equation}
The conditional path has the endpoints,
\begin{equation}
    \label{eq:conditional_path_endpoints}
    p_{01}(\x_t|\x_1) = p_0(\x_t) \quad \text{and} \quad p_{11}(\x_t|\x_1) = \delta(\x_t - \x_1),
\end{equation}
and is generated by its own conditional vector field $u_t(\x_t | \x_1)$ with the consistency condition,
\begin{equation}
    \label{eq:conditional_consistency}
    \frac{\partial p_{t1}(\x_t | \x_1)}{\partial t} = -\nabla \cdot (u_t(\x_t | \x_1) p_{t1}(\x_t | \x_1)).
\end{equation}

The conditional vector field $u_t(\x_t | \x_1)$ can be used to express the marginal vector field $u_t(\x_t)$ as,
\begin{equation}
    \label{eq:conditional_to_marginal}
    u_t(\x_t) = \int u_t(\x_t | \x_1) \frac{p_{t1}(\x_t | \x_1) p_1(\x_1)}{p_t(\x_t)} \diff \x_1.
\end{equation}
This is proven by,
\begin{equation}
    \begin{aligned}
    \frac{\partial p_t(\x_t)}{\partial t}
    &= \frac{\partial}{\partial t} \int p_{t1}(\x_t | \x_1) p_1(\x_1) \diff \x_1 \\
    &= \int \frac{\partial}{\partial t} p_{t1}(\x_t | \x_1) p_1(\x_1) \diff \x_1 \\
    &= - \int \left(\nabla \cdot (u_t(\x_t | \x_1) p_{t1}(\x_t | \x_1))\right) p_1(\x_1) \diff \x_1 \\
    &= - \nabla \cdot \left(\int u_t(\x_t | \x_1) p_{t1}(\x_t | \x_1) p_1(\x_1) \diff \x_1\right) \\
    &= - \nabla \cdot \left(\int u_t(\x_t | \x_1) \frac{p_{t1}(\x_t | \x_1) p_1(\x_1)}{p_t(\x_t)} p_t(\x_t) \diff \x_1\right) \\
    &= - \nabla \cdot \left(\left(\int u_t(\x_t | \x_1) \frac{p_{t1}(\x_t | \x_1) p_1(\x_1)}{p_t(\x_t)} \diff \x_1\right) p_t(\x_t) \right) \\
    &= - \nabla \cdot \left(u_t(\x_t) p_t(\x_t)\right),
    \end{aligned}
\end{equation}
which is the continuity equation for the marginal density $p_t(\x_t)$.
With this relations we can apply the same trick we did for score-matching, in that the flow matching (FM) loss,
\begin{equation}
    \label{eq:fm_loss}
    \mathcal{L}_{FM}(\theta) =
    \mathbb{E}_{t \sim \timeone}
    \mathbb{E}_{\x_t \sim p_t(\x_t)}
    \left|\left|u_\theta(\x_t) - u_t(\x_t)\right|\right|^2,
\end{equation}
and the CFM loss,
\begin{equation}
    \label{eq:cfm_loss}
    \mathcal{L}_{CFM}(\theta) =
    \mathbb{E}_{t \sim \timeone}
    \mathbb{E}_{\x_1 \sim \mathcal{D}}
    \mathbb{E}_{\x_0 \sim p_0}
    \left|\left|u_\theta(\x_t, t) - u_t(\x_t | \x_1)\right|\right|^2,
\end{equation}
are equivalent with respect to the model parameters~\cite{FlowMatchingGenerative}.

The only remaining feature is to describe the conditional probability path $p_{t1}(\x_t | \x_1)$.
The authors of~\cite{FlowStraightFast} propose a basic Gaussian probability path,
\begin{equation}
    \label{eq:conditional_gaussian}
    p_{t1}(\x_t | \x_1) = \mathcal{N}(\x_t; (1 - t)\x_1, t^2\mathbb{I}),
\end{equation}
which matches the relation in \Cref{eq:gaussian_kernel} with $s(t) = 1 - t$ and $\sigma(t) = t$ and allows for the same reparametrisation trick for sampling.
This corresponds to the conditional vector field,
\begin{equation}
    \label{eq:conditional_gaussian_field}
    u_t(\x_t | \x_1) = \frac{\x_t-\x_1}{t}.
\end{equation}
With these changes \Cref{eq:cfm_loss} simplifies even further with the linear interpolation scheme to
\begin{equation}
    \label{eq:linear_cfm_loss}
    \mathcal{L}_{CFM}(\theta) =
    \mathbb{E}_{t \sim \timeone}
    \mathbb{E}_{\x_1 \sim \mathcal{D}}
    \mathbb{E}_{\x_0 \sim p_0}
    \left|\left|u_\theta(\x_t, t) - (\x_1 - \x_0)\right|\right|^2,
\end{equation}
which is the most common form of the loss used in practice.

Once the network is trained, one can sample from the model by integrating the conditional vector field from noise sample to the data sample as with any CNF.

\subsection{Diffusion Models in Practice}

No matter the choice of framework, the training step for diffusion models largely the same.
One selects a time index $t$, samples from the data $\x \sim p_\Z$ and the latent distribution $\z \sim p_\X$, mixes them together to get the partially noised sample $\x_t = s(t)\x + \sigma(t)\z$ using predefined scaling factors, and passes both the noised sample and the time index through a neural network.

\subsubsection{Diffusion Frameworks}

Three diffusion frameworks are used in the various projects within this thesis.
The first uses SM objective by~\textcite{GenerativeModelingEstimating} with  trigonometric interpolations~\cite{StochasticInterpolants, ImprovedDenoisingDiffusion} which we refer to as SM-TI, the second follows the work by~\textcite{ElucidatingDesignSpace} often referred to as EDM diffusion, and the third is the CFM framework by~\textcite{FlowMatchingGenerative,FlowStraightFast,BuildingNormalizingFlows}.
Each of these frameworks differ can be seen as alternative hyperparameter selections for the diffusion schedule, network preconditioning, the training target, and the sampling procedures.
We briefly summarize the differences in the hyperparameters used in each of these frameworks in \Cref{tab:diffusion_frameworks}.

In SM-TI, the diffusion schedule was chosen to be a cosine and sine function such that the total power of $\x_t$, assuming that the training set was standardized, is constant across the schedule.
The network predicts the noise added to the data with the training objective
$\mathbb{E}_{\x, \z, t} \lambda(t) \left|\left| \e_\theta(\x_t, t) - \z\right|\right|^2$.
This is then used to approximate the score function, which is used in either the reverse SDE or the probability flow ODE to generate samples.

In EDM the diffusion schedule was constructed to simplify the probability flow ODE as much as possible, resulting in noise being added at a constant rate to a signal that was kept the same.
This results in straighter trajectories, which is beneficial for the numerical integration during sampling as it reduces the truncation error.
It also means that the limit of the time index set the maximum noise level.
The network is trained to predict the data itself, with the training objective $\mathbb{E}_{\x, \z, t} \lambda(t) \left|\left| f_\theta(\x_t, t) - \x\right|\right|^2$.
The actual denoising model is parametrized such that
\begin{equation}
    f_\theta = c_\text{skip}(t) \x_t + c_\text{out}(t)F_\theta(c_\text{in}(t)\x_t, c_\text{noise}(t)),
\end{equation}
where $F_\theta$ is the neural network and $c_\text{skip}, c_\text{in}, c_\text{out}, c_\text{noise}$ are manually selected scaling factors such that the training objective and the network's inputs have unit variance across all timesteps.
With the ODE, one can simply take steps towards the estimated data sample during integration.

In the CFM framework, the diffusion schedule is chosen to be a basic linear interpolation between the noise and the data.
The training target is simply the difference between the two.

A natural question is which of these models is superior.
Research on the topic is still ongoing, and it takes time for the community to reach a consensus.
That being said, the CFM framework with the basic linear diffusion schedule is now used by the majority of the commercial diffusion models used for text-to-image synthesis, including Stable Diffusion 3~\cite{SD3} and Flux~\cite{flux2024github}, indicating a preference for this framework in practice.

\begin{table}[h!]
    \centering
    \caption{Hyperparameters used in the different diffusion frameworks.}
    \label{tab:diffusion_frameworks}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    & \textbf{SM-TI} & \textbf{EDM} & \textbf{CFM} \\
    \midrule
    time index $t$ & $t \in [0, 1]$ & $t \in [0, 80]$ & $t \in [0, 1]$ \\
    signal rate $s(t)$ & $\cos(\frac{t\pi}{2})$ & $1$ & $1 - t$ \\
    noise rate $\sigma(t)$ & $\sin(\frac{t\pi}{2})$ & $t$ & $t$ \\
    latent distribution & $\normal$ & $\mathcal{N}(0, t_\text{max}\mathbb{I})$ & $\normal$ \\
    training target & noise - $\z$ & data - $\x$ & velocity - $(\x - \z)$ \\
    $\frac{\diff \x_t}{\diff t}$ & $- \frac{\pi}{2}\tan(\frac{\pi t}{2})(\x_t - \score)$ & $- t \score$ & $u_t(\x_t)$ \\
    sampling & SDE or ODE & ODE & ODE \\
    \bottomrule
    \end{tabular}
    }
\end{table}

Some examples of the generated samples from these models are shown in \Cref{fig:cm_samples}.
Here these samples were generated using the CNF framework to produce the double-moon and four-box datasets starting from a standard normal distribution.
One of the main benefits of the CNF framework is that the latent distribution does not have to be Gaussian at all, and the theory holds for any arbitrary distributions $p_0$ and $p_1$.
This is shown in the bottom row of \Cref{fig:cm_samples} where the model interpolates from the double-moon distribution to the four-box distribution.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/transformers/cm_samples.pdf}
    \caption{The transformation of gaussian samples into a double-moon distribution (top) and a four-box distribution (middle), and the interpolation between the two (bottom). The samples were generated using the CNF framework using Heun's method~\cite{heun} for 64 steps. Each column shows the samples at a different time index $t$, the black lines trace back to the samples in the previous column. The rightmost column shows the full paths traced by each sample from the base distribution.}
    \label{fig:cm_samples}
\end{figure}


\subsubsection{Conditional Verses Unconditional Paths}

One of the claims of both EDM and CFM is that they lead to straighter trajectories than SM-TI, allowing for faster sampling as one needs to perform less steps in the numerical integration.
This is indeed the case as shown by \Cref{fig:vp_vs_cfm} where the SM-TI model results in paths with very high curvature.
Note that the endpoints, the final distribution, is the same for both, yet neither paths the CFM are still not quite straight.
The reason for this is that all frameworks were derived using a similar argument; that the marginal density (velocity) is intractable, but one can use the conditional density (conditional velocity) as a proxy.
This disconnect also means that there is tension between the actual training target and what the model tends towards, highlighted in \Cref{fig:squares_gaussian} where a CNF was trained to generate samples from a two-box distribution.
This especially true for when training with non-Gaussian latent distributions as shown in \Cref{fig:oned_oned}, where the model was trained to interpolate between the same distribution even though an optimal model in this case would have $u_t(\x_t) = 0$.
So even if the conditional paths are straight, as with EDM and CFM, he marginal paths are not.
This discrepancy results in a few considerations for training.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/transformers/fm.png}
    \includegraphics[width=0.45\textwidth]{Figures/transformers/vp.png}
    \caption{The marginal paths generated interpolating between a Gaussian and a double-moon distribution learned by using the CFM schedule (left) and the SM-TI schedule (right).}
    \label{fig:vp_vs_cfm}
\end{figure}

The using the conditional paths as a proxy means that updates are unbiased but have high variance.
So while diffusion models arent at a risk to collapse like GANs while training, gradient updates are noisy and convergence is slow.
Almost all diffusion models in practice are trained with two copies of the network, and online and a offline network.
The online network is updated using the loss term with standard gradient descent, while the offline network is updated using the exponential moving average of the online network.
The offline model is always used for sampling.
This is a common trick which could be applied to any deep learning task~\cite{Adam} but it is ubiquitous in diffusion models.

Note that for CFM predicting the marginal velocity is significantly harder in the middle of the schedule as opposed to the endpoints.
This is because the optimal prediction for the marginal velocity at $t=0$ is to point towards the mean of $p_1$, and the optimal prediction at $t=1$ is to point towards the mean of $p_0$.
Likewise for EDM, when no noise has been added the model should simply return the input, and when the noise is at it's maximum the model can not do anything more but return the mean of the training set.
Thus it is common to use a non uniform sampling for the time index during training, to target the timesteps where the most information can be gained.
These are typically based on empirical results, and are not derived from the theory.
For EDM, $\log(t) \sim \mathcal{N}(-1.2, 1.2)$, is commonly used, where the specific values were chosen after a hyperparameter search~\cite{ElucidatingDesignSpace}.
Likewise for CFM, it was found that logit-normal sampling,
\begin{equation}
    t \sim \frac{1}{\sqrt{2\pi}t(1-t)}\exp\left(-\frac{(\text{logit}(t))^2}{2}\right),
\end{equation}
where $\text{logit}(t) = \log\left(\frac{t}{1-t}\right)$, was the more effective than uniform sampling~\cite{SD3}.

Even with these tricks, the highly variant outputs are still a problem and much work has been dedicated to learning straighter paths.
Perfectly strait paths would be optimal, as they could be solved using a single Euler step.
One approach is to use mini-batch optimal transport couplings between the data and the latent samples before mixing~\cite{ImprovingGeneralizingFlowbased}.
However, even for mini-batches this method does not scale well to high dimensional data.
Another approach is to take a trained model and \textit{distill} it, by training a second model to learn the same endpoints as the first model, but with straighter trajectories.
There are various methods for distillation, such as rectification~\cite{FlowStraightFast}, progressive distillation~\cite{ProgressiveDistillationFast}, and consistency distillation~\cite{ConsistencyModels}.
Each method attempts to reduce the number of inference steps required to generate high quality samples.

There is however an argument to be made that it is the multiple steps that make diffusion models so powerful.
Since the process requires removing noise from the data, any additional error terms induced by truncation errors are also removed.
Diffusion models have been described as applying iterative refinements~\cite{ImageSuperResolutionIterative} to the data, and the multiple steps might be what is allowing the model to self correct.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{Figures/transformers/oned_target.png}
    \includegraphics[width=0.49\textwidth]{Figures/transformers/oned.png}
    \caption{(left) The conditional probability paths used to train a CFM, and (right) the marginal paths generated by the model for a two-box distribution mapping to a Gaussian.
    }
    \label{fig:squares_gaussian}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{Figures/transformers/oned2.png}
    \includegraphics[width=0.49\textwidth]{Figures/transformers/oned2_marginal.png}
    \caption{(left) The conditional probability paths used to train a CFM, and (right) the marginal paths generated by the model for interpolating between the same three-box distribution.
    }
    \label{fig:squares_gaussian}
\end{figure}

\subsubsection{Time Encoding}

In all models the network needs to take in time information.
One can do this by simply concatenating the time index to the input, but it is more beneficial to perform positional encoding beforehand.
This is most commonly done using trigonometric functions, such as the positional encoding used in transformers~\cite{Attention}, or random Fourier features~\cite{FourierFeaturesLet}, but in our projects we have found the most success with a basic cosine-encoding (CosEnc)~\cite{ImplicitQuantileNetworks}.
This layer takes scalar $t$ and returns an output tensor of length $d$ where each element is the output of a cosine function an exponentially increasing frequency.
The lowest frequency is set such that the layer will not have duplicate outputs.
For parameters $t_\text{min}$, $t_\text{max}$, and $d$ the CosEnd layer is given by,
\begin{equation}
    \label{eq:cosine_encoding}
    \text{CosEnc}(t) =
    \begin{bmatrix}
        \cos\left(e^0 t'\right) \\
        \cos\left(e^1 t'\right) \\
        \vdots \\
        \cos\left(e^{d-1} t'\right)
    \end{bmatrix}, \quad \text{where} \quad t' = \pi \frac{t - t_\text{min}}{t_\text{max} - t_\text{min}} \\
\end{equation}

\subsubsection{Conditional Generation}

Other conditioning information $\con$ can easily be added to the network, such as class labels or other auxiliary information.
This works for all frameworks requires including the context information with the inputs to the network, the exact form depending on the architecture of the model.
Interestingly enough, one does not even have to train a conditional model to perform conditional generation due to the relation,
\begin{equation}
    \label{eq:conditional_generation}
    \nabla_{\x_t} \cdot \log p(\x_t | \con) = \nabla_{\x_t} \cdot \log p(\x_t) + \nabla_{\x_t} \cdot \log p(\con | \x_t).
\end{equation}
Thus, one only needs a second time-dependent classifier for the conditioning information.
Gradients with respect to this model can be used to modify the update rule for the ODE or the velocity field to perform conditional generation.
This is often referred to as \textit{guidance} in the diffusion literature~\cite{DiffusionBeatsGANS}.

\subsubsection{Sampling}

As previously discussed, various numerical integration techniques can be employed in conjunction with either the reverse Stochastic Differential Equation or the probability flow Ordinary Differential Equation to sample from a diffusion model. Beyond the conventional numerical integration methods, such as Euler, Heun, and Linear Multi-Step methods~\cite{heun}, a novel domain has emerged, focusing on the development of efficient integration methods tailored to specific diffusion schedules. Notable examples include DDIM~\cite{DDIM}, DPM-Solver~\cite{DPMSolverFastODE}, DPM-Solver++~\cite{DPMSolverFastSolver}, UniPC~\cite{UniPC}, and DPM-Solver-2M~\cite{DPMSolverFastSolver}. These samplers are adaptable to both the stochastic differential equation and ordinary differential equation frameworks.

Furthermore, the number of integration steps and the step size vary across different methods. In~\cite{ElucidatingDesignSpace}, the integration steps were designed to take larger steps at the beginning of the schedule and smaller steps towards the end, facilitating refinement. This variability complicates the comparison of models, as the same model may yield different results depending on the integration method employed. This presents a significant challenge in the field.
